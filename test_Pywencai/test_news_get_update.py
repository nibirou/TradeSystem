import time
import random
from typing import List, Dict, Optional
import requests

from bs4 import BeautifulSoup
import re
import pandas as pd

class IwencaiNewsClientSafe:
    """
    é£æ§å‹å¥½å‹ï¼šä½é¢‘ã€é™é¡µã€é€€é¿ã€é‡403ç†”æ–­ã€‚
    """

    URL = "https://www.iwencai.com/unifiedwap/unified-wap/v1/information/news"

    def __init__(
        self,
        cookie_str: str,
        min_interval_sec: float = 6.0,
        max_pages_per_stock: int = 5,
        page_size: int = 15,
        cooldown_403_sec: int = 15 * 60,
        timeout_sec: int = 10,
        user_agent: Optional[str] = None,
    ):
        # âœ… ä¸€å®šè¦å…ˆåˆ›å»º session
        self.session = requests.Session()

        self.cookie_str = cookie_str.strip()
        self.min_interval_sec = float(min_interval_sec)
        self.max_pages_per_stock = int(max_pages_per_stock)
        self.page_size = int(page_size)
        self.cooldown_403_sec = int(cooldown_403_sec)
        self.timeout_sec = int(timeout_sec)

        ua = user_agent or (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/143.0.0.0 Safari/537.36"
        )

        # âœ… headers å›ºå®šä¸‹æ¥ï¼Œæ¨¡æ‹ŸçœŸå®é¡µé¢è¯·æ±‚
        self.session.headers.update({
            # "Accept": "application/json, text/plain, */*",
            "Content-Type": "application/x-www-form-urlencoded",
            "Origin": "https://www.iwencai.com",
            "Referer": "https://www.iwencai.com/unifiedwap/inforesult",
            "User-Agent": ua,
            "Cookie": self.cookie_str,
            # "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9",
            # # âœ… å…³é”®ï¼šç¦æ­¢ br / zstdï¼Œé¿å… requests è§£ä¸å‡ºæ¥å¯¼è‡´â€œä¹±ç â€
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
        })

        self._last_request_ts = 0.0

        # âœ… åˆå§‹åŒ–è‡ªæ£€ï¼šç¡®ä¿ session å­˜åœ¨
        if not hasattr(self, "session") or self.session is None:
            raise RuntimeError("Session åˆå§‹åŒ–å¤±è´¥ï¼šself.session is None")

    def _rate_limit_and_human_pause(self):
        """
        1) ä¿è¯ä¸¤æ¬¡è¯·æ±‚ä¹‹é—´ >= min_interval
        2) å¢åŠ è½»å¾®éšæœºåœé¡¿ï¼Œæ¨¡æ‹Ÿâ€œé˜…è¯»/ç¿»é¡µâ€
        """
        now = time.time()
        dt = now - self._last_request_ts
        if dt < self.min_interval_sec:
            time.sleep(self.min_interval_sec - dt)

        # è½»å¾®éšæœºï¼šä¸è¦æŠ–å¤ªå¤§ï¼ŒæŠ–å¤ªå¤§ä¹Ÿä¸åƒäºº
        time.sleep(random.uniform(1.0, 3.0))
        self._last_request_ts = time.time()

    def _post(self, data: Dict) -> requests.Response:
        """
        å¸¦é€€é¿çš„ POSTï¼š
        - 403ï¼šç†”æ–­å†·å´
        - 429/5xxï¼šæŒ‡æ•°é€€é¿
        """
        backoff = 2.0
        for attempt in range(1, 6):  # æœ€å¤šå°è¯• 5 æ¬¡
            self._rate_limit_and_human_pause()
            resp = self.session.post(self.URL, data=data, timeout=self.timeout_sec)

            # 403ï¼š confirmed é£æ§
            if resp.status_code == 403:
                print(f"âš ï¸ 403 Forbiddenï¼ˆattempt={attempt}ï¼‰ï¼Œå†·å´ {self.cooldown_403_sec}s åé€€å‡º")
                time.sleep(self.cooldown_403_sec)
                raise RuntimeError("è§¦å‘ 403 é£æ§ï¼Œå·²å†·å´å¹¶ç»ˆæ­¢æœ¬æ¬¡ä»»åŠ¡")

            # 429 æˆ– 5xxï¼šé€€é¿å†è¯•ï¼ˆä¸ç¡¬æ’ï¼‰
            if resp.status_code == 429 or 500 <= resp.status_code < 600:
                sleep_s = backoff + random.uniform(0.0, 1.5)
                print(f"âš ï¸ HTTP {resp.status_code}ï¼ˆattempt={attempt}ï¼‰ï¼Œé€€é¿ {sleep_s:.1f}s åé‡è¯•")
                time.sleep(sleep_s)
                backoff *= 1.8
                continue

            resp.raise_for_status()
            return resp

        raise RuntimeError("å¤šæ¬¡é‡è¯•ä»å¤±è´¥ï¼ˆ429/5xxï¼‰")

    # >>> ADD
    def _gen_random_userid_like(self, old_userid: str) -> str:
        """
        ç”Ÿæˆä¸€ä¸ªâ€œä½æ•°ç›¸åŒâ€çš„éšæœº useridï¼ˆä»…ç”¨äºä¸€æ¬¡æ€§éªŒè¯ï¼‰
        """
        n = len(old_userid)
        # é¿å…å‰å¯¼ 0
        first = str(random.randint(1, 9))
        rest = "".join(str(random.randint(0, 9)) for _ in range(n - 1))
        return first + rest

    def _replace_userid_in_cookie(self, new_userid: str):
        """
        åªæ›¿æ¢ Cookie ä¸­çš„ userid å­—æ®µ
        """
        parts = self.cookie_str.split(";")
        new_parts = []
        replaced = False

        for p in parts:
            p_strip = p.strip()
            if p_strip.startswith("userid="):
                new_parts.append(f" userid={new_userid}")
                replaced = True
            else:
                new_parts.append(p)

        if not replaced:
            # ç†è®ºä¸Šä¸ä¼šå‘ç”Ÿï¼Œä½†å…œåº•
            new_parts.append(f" userid={new_userid}")

        self.cookie_str = ";".join(new_parts).strip()
        self.session.headers["Cookie"] = self.cookie_str

    def fetch_news_page(
        self,
        code: str,
        offset: int = 0,
        dl: int = 120,
        tl: int = 41,
        date_range: int = 3,
    ) -> Dict:
        payload = {
            "query": code,
            "size": str(self.page_size),
            "offset": str(offset),
            "dl": str(dl),
            "tl": str(tl),
            "date_range": str(date_range),
            "mobile": "3",
        }

        resp = self._post(payload)
        data = resp.json()

        # >>> MODIFYï¼šæ•è·â€œæŸ¥è¯¢ç»“æœä¸ºç©ºâ€
        if data.get("status_code") != 0:
            msg = data.get("status_msg", "")
            if "æŸ¥è¯¢ç»“æœä¸ºç©º" in msg:
                print("âš ï¸ è¿”å›ç©ºç»“æœï¼Œå°è¯•æ›´æ¢ userid è¿›è¡Œä¸€æ¬¡éªŒè¯")

                # è§£æå½“å‰ userid
                old_userid = None
                for p in self.cookie_str.split(";"):
                    p = p.strip()
                    if p.startswith("userid="):
                        old_userid = p.split("=", 1)[1]
                        break

                if old_userid:
                    new_userid = self._gen_random_userid_like(old_userid)
                    print(f"   userid: {old_userid} -> {new_userid}")
                    self._replace_userid_in_cookie(new_userid)

                    # ğŸ” ä»… retry ä¸€æ¬¡
                    resp2 = self._post(payload)
                    data2 = resp2.json()

                    if data2.get("status_code") == 0:
                        return data2["data"]

                # retry ä»å¤±è´¥ï¼ŒæŠ›å¼‚å¸¸
                raise RuntimeError(f"æ¥å£è¿”å›å¼‚å¸¸ï¼ˆretry åï¼‰ï¼š{msg}")

            raise RuntimeError(f"æ¥å£è¿”å›å¼‚å¸¸ï¼š{msg}")

        return data["data"]


    def crawl_stock_news(self, code: str) -> List[Dict]:
        """
        å•è‚¡ç¥¨æŠ“å–ï¼šä¸¥æ ¼é™åˆ¶é¡µæ•°ï¼Œé¿å…è§¦å‘é£æ§ã€‚
        """
        results_all: List[Dict] = []
        offset = 0

        for page in range(self.max_pages_per_stock):
            data = self.fetch_news_page(code=code, offset=offset)
            results = data.get("results", [])
            if not results:
                break

            results_all.extend(results)
            offset += len(results)

        return results_all

    def _decode_html_bytes(self, raw: bytes, header_ct: str | None = None) -> str:
        """
        å¯¹ bytes åšç¨³å¥è§£ç ï¼šä¼˜å…ˆ header charsetï¼Œå…¶æ¬¡å¸¸è§ä¸­æ–‡ç¼–ç ï¼Œå†ç”¨ utf-8 å…œåº•ã€‚
        """
        # 1) header é‡Œæœ‰ charset
        if header_ct:
            m = re.search(r"charset=([-\w]+)", header_ct, re.I)
            if m:
                enc = m.group(1).strip().lower()
                try:
                    return raw.decode(enc, errors="replace")
                except Exception:
                    pass

        # 2) å¸¸è§ä¸­æ–‡ç«™ç‚¹ç¼–ç ä¼˜å…ˆå°è¯•
        for enc in ("utf-8", "gb18030", "gbk", "gb2312"):
            try:
                txt = raw.decode(enc, errors="replace")
                # ç®€å•åˆ¤å®šï¼šè§£å‡ºæ¥å«å¤§é‡ä¸­æ–‡æ‰æ›´å¯ä¿¡ï¼ˆå¯é€‰ï¼‰
                return txt
            except Exception:
                continue

        # 3) æœ€åå…œåº•
        return raw.decode("utf-8", errors="replace")

    def fetch_full_article(self, url: str) -> str:
        if not url:
            return ""
        if url.startswith("//"):
            url = "https:" + url

        # âœ… æ›´åƒçœŸå®ç”¨æˆ·ï¼šæ­£æ–‡é¡µæ¯”æ¥å£æ…¢å¾ˆå¤š
        time.sleep(random.uniform(3.0, 6.0))

        resp = self.session.get(url, timeout=15, allow_redirects=True)
        # 403 / 429 ä¹Ÿåˆ«ç¡¬æ’ï¼ˆä½ å¯ä»¥æ²¿ç”¨ä½ å·²æœ‰çš„ç†”æ–­é€»è¾‘ï¼‰
        resp.raise_for_status()

        # âœ… å…³é”®ï¼šä¸è¦ç”¨ resp.textï¼ˆå®¹æ˜“ä¹±ç ï¼‰
        html = self._decode_html_bytes(resp.content, resp.headers.get("Content-Type"))

        soup = BeautifulSoup(html, "lxml")

        # 1) article
        article = soup.find("article")
        if article:
            return self._clean_text(article.get_text("\n", strip=True))

        # 2) å¸¸è§æ­£æ–‡å®¹å™¨ï¼ˆè¦†ç›–å¾®ä¿¡/è´¢ç»ç«™å¸¸è§ç»“æ„ï¼‰
        candidates = [
            {"id": "js_content"},               # å¾®ä¿¡æ­£æ–‡
            {"class_": re.compile(r"rich_media_content")},
            {"class_": re.compile(r"article-content|articleContent|content|main-content|post-content|article_content")},
            {"class_": re.compile(r"TRS_Editor|article|detail|text")},  # ä¸€äº›é—¨æˆ·
        ]
        for kw in candidates:
            node = soup.find(**kw)
            if node:
                txt = node.get_text("\n", strip=True)
                if len(txt) > 80:
                    return self._clean_text(txt)

        # 3) å…œåº•ï¼šèšåˆ p
        ps = soup.find_all("p")
        if ps:
            texts = []
            for p in ps:
                t = p.get_text(strip=True)
                if len(t) >= 20:
                    texts.append(t)
            if texts:
                return self._clean_text("\n".join(texts))

        return ""

    def _clean_text(self, text: str) -> str:
        """
        ç®€å•æ¸…æ´—æ­£æ–‡
        """
        if not text:
            return ""
        text = re.sub(r"\n{3,}", "\n\n", text)
        text = re.sub(r"\s{2,}", " ", text)
        return text.strip()
    
    def export_news_to_csv(self, news_list: List[Dict], csv_path: str):
        """
        å¯¹æ¯æ¡æ–°é—»æŠ“å–å…¨æ–‡å¹¶å¯¼å‡º CSV
        """
        rows = []

        for i, n in enumerate(news_list, 1):
            print(f"ğŸ“„ æŠ“å–æ­£æ–‡ {i}/{len(news_list)}")

            full_content = self.fetch_full_article(n.get("url", ""))

            rows.append({
                "publish_time": n.get("publish_time", ""),
                "publish_source": n.get("publish_source", ""),
                "title": n.get("title", ""),
                "summary": n.get("summary", ""),
                "full_content": full_content,
                "source_url": n.get("url", ""),
            })

        df = pd.DataFrame(rows)
        df.to_csv(csv_path, index=False, encoding="utf-8-sig")
        print(f"âœ… CSV å·²è¾“å‡º: {csv_path}")



if __name__ == "__main__":
    # âœ… æŠŠä½ é‚£æ¡ cookie æ‹¼æˆä¸€è¡Œå­—ç¬¦ä¸²ç²˜è´´åœ¨è¿™é‡Œï¼ˆåŸæ ·å³å¯ï¼‰
    COOKIE_STR = (
                "cuc=xx8ezzz2zvmw; "
                "escapename=mx_566796434; "
                "u_name=mx_566796434; "
                "ta_random_userid=ktsynuvtgz; "
                "sess_tk=eyJ0eXAiOiJKV1QiLCJhbGciOiJFUzI1NiIsImtpZCI6InNlc3NfdGtfMSIsImJ0eSI6InNlc3NfdGsifQ."
                "eyJqdGkiOiI2ZTAwYTQ4MzE4MTdjNjlhZGUwZjEzZWY2MjRmMTFkNTEiLCJpYXQiOjE3NjgyMTAwNzYsImV4cCI6"
                "MTc2ODgxNDg3Niwic3ViIjoiNTY2Nzk2NDM0IiwiaXNzIjoidXBhc3MuaXdlbmNhaS5jb20iLCJhdWQiOiIyMDIw"
                "MTExODUyODg5MDcyIiwiYWN0Ijoib2ZjIiwiY3VocyI6IjNjZGIzNWNiOTdmZmQ2Mzk0M2U3OTdiMGJmNzg2NjY4"
                "ZDEzOGNhZGU0Mzg0N2IyMjI1NjkyZTVlYWMzMzA0NmMifQ."
                "4fJThGiPP8-Vsm_HAbRoS9v81mX6jTxa5riLlJDuoLrBPzafG8YzJl2OtrSFtcZeITMRzJvxCZF03Cy1IIw5qw; "
                "ticket=a7f38de4cf7f3dee19d63006e77965c3; "
                "ttype=WEB; "
                "u_ttype=WEB; "
                "other_uid=Ths_iwencai_Xuangu_9t4ghwjskfabmu9iy7daondbz4hnjia0; "
                "user=MDpteF81NjY3OTY0MzQ6Ok5vbmU6NTAwOjU3Njc5NjQzNDo3LDExMTExMTExMTExLDQwOzQ0LDExLDQwOzYs"
                "MSw0MDs1LDEsNDA6MTY6Ojo1NjY3OTY0MzQ6MTc2ODIxMDA3Njo6OjE2MTI5MjUzNDA6NjA0ODAwOjA6MWQ1M"
                "TE0ZjYyZWYxMzBmZGU5YWM2MTcxODgzYTQwMDZlOmRlZmF1bHRfNTow; "
                "u_ukey=A10702B8689642C6BE607730E11E6E4A; "
                "v=A090c19BeiKTqX5e7o9euutl3uhcdKKgPcmnmWFc6oDGeGGWaUQz5k2YN81y; "
                "PHPSESSID=96090d860f932f660d55e0b685d0442d; "
                "userid=362796439; "    # æœ€å¥½æ˜¯æ¯æ¢ä¸€åªè‚¡ç¥¨éšæœºç”Ÿæˆä¸€ä¸ªuserid
                "utk=3262e41d288421bc9c6340644220039b; "
                "cid=2c9c0c9b495a5e33e6226c22e7cc3ed91768187607; "
                "ComputerID=2c9c0c9b495a5e33e6226c22e7cc3ed91768187607; "
                "u_dpass=2CcKkH00sroyYH%2FsI12MMJvbN4IRtVjK3sUwYW%2F1mHvt%2B8LjA3QxTinPHALynlBfHi80LrSsTFH9a%2B6rtRvqGg%3D%3D; "
                "u_did=233EB705EF0C4596BD1A255BE0753091; "
                "u_uver=1.0.0; "
                "WafStatus=1; "
                "user_status=0"
            )

    client = IwencaiNewsClientSafe(
        cookie_str=COOKIE_STR,
        min_interval_sec=8.0,        # æ›´ä¿å®ˆä¸€ç‚¹
        max_pages_per_stock=1,       # å•æ¬¡æœ€å¤š 3 é¡µ
        cooldown_403_sec=15 * 60,    # 403 å†·å´ 15 åˆ†é’Ÿ
    )

    stock_code = "600223"  # âœ… åœ¨è¿™é‡ŒæŒ‡å®š
    try:
        news_list = client.crawl_stock_news(stock_code)
        print(f"æŠ“å–å®Œæˆï¼š{stock_code} å…± {len(news_list)} æ¡\n")

        for n in news_list[:5]:
            print(n.get("publish_time"), n.get("publish_source"))
            print(n.get("title"))
            print("-" * 80)
        
        client.export_news_to_csv(
            news_list,
            csv_path=f"news_{stock_code}.csv"
        )

    except Exception as e:
        print("ç»ˆæ­¢åŸå› ï¼š", e)
