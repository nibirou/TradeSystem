# -*- coding: utf-8 -*-
"""
ä¸ªè‚¡ æ¶ˆæ¯é¢ + èµ„é‡‘é¢ æ•°æ®æå–ï¼ˆå•æ–‡ä»¶ Â· æ­£æ–‡æŠ“å–å‡çº§ç‰ˆï¼‰
- æ–°é—»æ ‡é¢˜åˆ—åé²æ£’æ€§ + æ—¥æœŸæ’åº
- æ­£æ–‡æŠ“å–ï¼šä¼˜å…ˆç”¨â€œæ–°é—»é“¾æ¥â€è·å–å®Œæ•´æ–‡ç« å†…å®¹ï¼ˆå¤šç­–ç•¥ + ç«™ç‚¹ç‰¹åŒ–ï¼‰
- æ­£æ–‡/æ–°é—»ç¼“å­˜ï¼ˆé¿å…é‡å¤æŠ“ï¼‰
- ç¼“å­˜è¿‡æœŸåˆ·æ–°ï¼ˆæ–°é—»åˆ—è¡¨/èµ„é‡‘æµ/æ­£æ–‡ï¼‰
- é‡è¯• + æŒ‡æ•°é€€é¿ + å¤±è´¥æ—¥å¿—
- å¹¶å‘æé€Ÿ + è½»é‡é™é€Ÿ
"""

import os
import re
import time
import json
import hashlib
import random
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed

import pandas as pd
import akshare as ak
from snownlp import SnowNLP
from tqdm import tqdm

# â€”â€” å¯é€‰ä¾èµ–ï¼šè‡ªåŠ¨ä¼˜å…ˆä½¿ç”¨ï¼Œç¼ºå°‘åˆ™è‡ªåŠ¨é™çº§ â€”â€”
try:
    from newspaper import Article  # pip install newspaper3k
    _HAS_NEWSPAPER = True
except Exception:
    _HAS_NEWSPAPER = False

try:
    from readability import Document  # pip install readability-lxml
    _HAS_READABILITY = True
except Exception:
    _HAS_READABILITY = False

try:
    import requests
    from bs4 import BeautifulSoup  # pip install beautifulsoup4 lxml
    _HAS_REQUESTS = True
except Exception:
    _HAS_REQUESTS = False

try:
    import trafilatura  # pip install trafilatura
    _HAS_TRAFILATURA = True
except Exception:
    _HAS_TRAFILATURA = False

try:
    from boilerpy3 import extractors as _bp_extractors
    _HAS_BOILERPY3 = True
except Exception:
    _HAS_BOILERPY3 = False

try:
    from goose3 import Goose
    _HAS_GOOSE3 = True
except Exception:
    _HAS_GOOSE3 = False   


# =================== å¯é…ç½®å‚æ•° ===================
R00T_DIR = "data"
META_DIR = "data/metadata"
STOCK_NEWS_DIR = os.path.join(R00T_DIR, "stock_news")
RESULT_DIR = os.path.join(STOCK_NEWS_DIR, "news_nlp_result_by_day")
os.makedirs(META_DIR, exist_ok=True)
os.makedirs(RESULT_DIR, exist_ok=True)

RUN_TS = datetime.now().strftime("%Y%m%d_%H%M%S")
OUT_PATH = os.path.join(RESULT_DIR, f"stock_news_factors_{RUN_TS}.csv") # æ–°é—»è¯„åˆ†ç»“æœè¾“å‡º(å¸¦æ—¥æœŸ)
FAIL_LOG = os.path.join(RESULT_DIR, f"stock_news_failures_{RUN_TS}.log") # æ–°é—»è·å–å¤±è´¥ç»“æœï¼ˆå¸¦æ—¥æœŸï¼‰

# Akshare ä¸œè´¢ä¸ªè‚¡æ–°é—»æ¥å£
AKSHARE_STOCK_NEWS_EM_DIR = os.path.join(STOCK_NEWS_DIR, "akshare_stock_news_em")
AKSHARE_STOCK_NEWS_EM_ARTICLES_DIR = os.path.join(AKSHARE_STOCK_NEWS_EM_DIR, "articles")     # æ­£æ–‡ç¼“å­˜ç›®å½•
os.makedirs(AKSHARE_STOCK_NEWS_EM_DIR, exist_ok=True)
os.makedirs(AKSHARE_STOCK_NEWS_EM_ARTICLES_DIR, exist_ok=True)

# å…¶ä»–æ–¹æ³•/æ¥å£ç›®å½•


# ç¼“å­˜æ–‡ä»¶æœ€å¤§æœ‰æ•ˆæœŸï¼ˆå°æ—¶ï¼‰ï¼Œè¶…è¿‡åˆ™åˆ·æ–°æŠ“å–
MAX_AGE_HOURS_NEWS = 24        # æ–°é—»åˆ—è¡¨ç¼“å­˜
MAX_AGE_HOURS_ARTICLE = 24*7   # æ­£æ–‡ç¼“å­˜ï¼šä¸€å‘¨

# æŠ“å–é‡è¯•
RETRIES = 3
BASE_WAIT = 0.8  # ç§’ï¼ŒæŒ‡æ•°é€€é¿åŸºæ•°

# å¹¶å‘çº¿ç¨‹æ•°
MAX_WORKERS = 8

# è½»é‡é™é€Ÿï¼šæ¯åªè‚¡ç¥¨æŠ“å®Œåéšæœº sleep åŒºé—´ï¼ˆç§’ï¼‰
RANDOM_SLEEP_RANGE = (0.05, 0.2)

# æ–°é—»æƒ…ç»ªï¼šå–â€œæœ€è¿‘ topk æ¡â€ç”¨äºæƒ…ç»ªæ‰“åˆ†
NEWS_TOPK = 20

# æ­£æ–‡æœ€å°é•¿åº¦é˜ˆå€¼ï¼ˆå¤ªçŸ­è¯´æ˜æŠ½å–å¤±è´¥ï¼Œå›é€€æ ‡é¢˜/æ‘˜è¦ï¼‰
MIN_ARTICLE_CHARS = 60
# å¯¹è¶…é•¿æ­£æ–‡å¯æˆªæ–­åšæƒ…æ„Ÿï¼ˆé¿å…è¶…æ…¢ï¼‰ï¼ŒæŒ‰å­—ç¬¦æˆªæ–­
MAX_ARTICLE_CHARS = 6000

# Requests æŠ“å–å‚æ•°
REQ_TIMEOUT = 12
REQ_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/124.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "zh-CN,zh;q=0.9",
}


# =================== å·¥å…·å‡½æ•° ===================
def _format_pub_time(val) -> str:
    """
    å°†å‘å¸ƒæ—¶é—´å€¼ç»Ÿä¸€æ ¼å¼åŒ–ä¸º YYYYMMDD_HHMMSSï¼›å¤±è´¥åˆ™è¿”å› 'unknown'
    """
    try:
        ts = pd.to_datetime(val, errors="coerce")
        if pd.isna(ts):
            return "unknown"
        return ts.strftime("%Y%m%d_%H%M%S")
    except Exception:
        return "unknown"
    
def _need_refresh(path: str, max_age_hours: int) -> bool:
    if not os.path.exists(path):
        return True
    try:
        mtime = datetime.fromtimestamp(os.path.getmtime(path))
        return (datetime.now() - mtime).total_seconds() > max_age_hours * 3600
    except Exception:
        return True


def _retry(fn, tries=RETRIES, base_wait=BASE_WAIT):
    last_err = None
    for i in range(tries):
        try:
            return fn()
        except Exception as e:
            last_err = e
            if i < tries - 1:
                time.sleep(base_wait * (2 ** i) + random.random() * 0.3)
    raise last_err


def _log_fail(msg: str):
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(FAIL_LOG, "a", encoding="utf-8") as f:
        f.write(f"[{ts}] {msg}\n")


def _random_pause():
    low, high = RANDOM_SLEEP_RANGE
    time.sleep(random.uniform(low, high))


def _get_first_existing_col(df: pd.DataFrame, candidates):
    for c in candidates:
        if c in df.columns:
            return c
    return None


def _to_datetime_sorted(df: pd.DataFrame, date_candidates=("æ—¥æœŸ", "æ—¶é—´", "å‘å¸ƒæ—¶é—´", "time", "pub_time")):
    """
    æ‰¾åˆ°ä¸€ä¸ªæ—¥æœŸåˆ—ï¼Œè½¬æ¢ä¸º datetime å¹¶æŒ‰å‡åºæ’åºï¼›è‹¥æ‰¾åˆ°å¤šåˆ—ï¼Œå–ç¬¬ä¸€ä¸ª
    """
    for c in date_candidates:
        if c in df.columns:
            try:
                df[c] = pd.to_datetime(df[c], errors="coerce")
                df = df.sort_values(c, ascending=True)  # å‡åºï¼Œä¾¿äº tail(N) å–æœ€è¿‘ N æ¡
                return df
            except Exception:
                continue
    return df  # æ‰¾ä¸åˆ°æ—¥æœŸåˆ—å°±åŸæ ·è¿”å›


def _md5(s: str) -> str:
    return hashlib.md5(s.encode("utf-8")).hexdigest()

# â€”â€” å¸¸è§â€œå…è´£å£°æ˜/å£°æ˜/ç‰ˆæƒå°¾å·´â€å…³é”®è¯ â€”â€” 
_DISCLAIMER_PREFIXES = [
    "éƒ‘é‡å£°æ˜", "ç‰¹åˆ«å£°æ˜", "å…è´£å£°æ˜", "é£é™©æç¤º"
]
_DISCLAIMER_KEYWORDS = [
    "ä¸æ„æˆæŠ•èµ„å»ºè®®", "ä»…ä¾›å‚è€ƒ", "ä¸œæ–¹è´¢å¯Œç½‘ä¸å¯¹", "ä»¥ä¸­å›½è¯ç›‘ä¼šæŒ‡å®š",
    "ç‰ˆæƒ", "è½¬è½½", "è´£ä»»ç¼–è¾‘", "æ¥æºï¼šä¸œæ–¹è´¢å¯Œ"
]

def _looks_like_disclaimer(p: str) -> bool:
    if not p: 
        return False
    p = p.strip()
    if any(p.startswith(pre) for pre in _DISCLAIMER_PREFIXES):
        return True
    if len(p) < 120 and any(k in p for k in _DISCLAIMER_KEYWORDS):
        # æ®µè½å¾ˆçŸ­è€Œä¸”åŒ…å«å£°æ˜å…³é”®è¯ï¼Œæ›´åƒå£°æ˜/ç‰ˆæƒå°¾å·´æˆ–æç¤ºæ¡
        return True
    return False

def _strip_disclaimer_tail(txt: str) -> str:
    if not txt:
        return txt
    # ä»å‡ºç°å£°æ˜å…³é”®è¯çš„ä½ç½®æˆªæ–­ï¼ˆä¿å®ˆåšæ³•ï¼šä»…åœ¨é åæ—¶æˆªï¼‰
    for kw in _DISCLAIMER_PREFIXES + _DISCLAIMER_KEYWORDS:
        idx = txt.rfind(kw)
        if idx != -1 and idx > len(txt) * 0.6:
            return txt[:idx].strip()
    return txt

def _clean_join_paragraphs(paras):
    """åˆå¹¶æ®µè½å¹¶å»æ‰å£°æ˜/å·¥å…·æ¡/å¾ˆçŸ­çš„UIæ–‡æ¡ˆ"""
    out = []
    for p in paras:
        p = re.sub(r"\s+", " ", p or "").strip()
        if not p: 
            continue
        if _looks_like_disclaimer(p):
            continue
        # è¿‡æ»¤åˆ†äº«ã€ç¼–è¾‘ã€æ”¶è—ã€æ‰“å°ç­‰å·¥å…·æ¡
        if len(p) < 12 and any(w in p for w in ["å¾®ä¿¡", "å¾®åš", "QQ", "æ”¶è—", "æ‰“å°", "å®¢æˆ·ç«¯", "APP", "åˆ†äº«"]):
            continue
        out.append(p)
    text = " ".join(out).strip()
    return _strip_disclaimer_tail(text)

def _score_candidate_text(t: str) -> float:
    """ç»™å¤šä¸ªæŠ½å–ç»“æœæ‰“åˆ†ï¼Œé€‰æœ€åƒâ€œæ­£æ–‡â€çš„é‚£ä¸ª"""
    if not t:
        return -1e9
    n = len(t)
    zh = sum(0x4e00 <= ord(c) <= 0x9fff for c in t)
    digits = sum(c.isdigit() for c in t)
    punct = sum(c in "ï¼Œã€‚ï¼›ï¼šï¼Ÿï¼,.:%ï¼›ã€" for c in t)
    has_disclaimer = any(k in t for k in _DISCLAIMER_PREFIXES + _DISCLAIMER_KEYWORDS)

    # é•¿åº¦ï¼ˆä¸Šé™ 5000ï¼‰ã€ä¸­æ–‡å æ¯”ã€æ˜¯å¦å«æ•°å­—&æ ‡ç‚¹ï¼ˆè´¢ç»ç¨¿å¸¸è§ï¼‰ã€å£°æ˜æƒ©ç½š
    score = min(n, 5000) / 5000.0
    if n > 0:
        score += 0.5 * (zh / n)
    if digits > 0:
        score += 0.15
    if punct > 5:
        score += 0.1
    if has_disclaimer and n < 400:
        score -= 1.0
    return score

def _sanitize_text(txt: str) -> str:
    if not txt:
        return ""
    # å»è„šæ³¨/é‡å¤ç©ºç™½
    txt = re.sub(r"\s+", " ", txt)
    # å¸¸è§ç‰ˆæƒå°¾å·´ç²—ç•¥å‰”é™¤ï¼ˆå¯æŒ‰éœ€æ‰©å±•ï¼‰
    txt = re.sub(r"(è´£ä»»ç¼–è¾‘[:ï¼š].*?$)", "", txt)
    return txt.strip()


# =================== æ­£æ–‡æŠ“å–ä¸ç¼“å­˜ ===================
def _article_cache_path(url: str, cache_key: str = None) -> str:
    """
    è‹¥æä¾› cache_keyï¼ˆå½¢å¦‚ '000001_20250815_180505'ï¼‰ï¼Œåˆ™ç”¨å®ƒå‘½åï¼›
    å¦åˆ™é€€å›ç”¨ url çš„ md5 å‘½åï¼ˆå‘åå…¼å®¹ï¼‰ã€‚
    """
    if cache_key:
        fname = f"{cache_key}.json"
    else:
        fname = f"{_md5(url)}.json"
    return os.path.join(AKSHARE_STOCK_NEWS_EM_ARTICLES_DIR, fname)

def _read_article_cache(url: str, cache_key: str = None):
    path = _article_cache_path(url, cache_key)
    if not os.path.exists(path):
        return None
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        ts = datetime.fromisoformat(data.get("fetched_at"))
        if (datetime.now() - ts).total_seconds() > MAX_AGE_HOURS_ARTICLE * 3600:
            return None
        return data.get("text", "")
    except Exception:
        return None


def _write_article_cache(url: str, text: str, cache_key: str = None):
    path = _article_cache_path(url, cache_key)
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(
                {"url": url, "text": text, "fetched_at": datetime.now().isoformat()},
                f,
                ensure_ascii=False
            )
    except Exception as e:
        _log_fail(f"WRITE_ARTICLE_CACHE_FAIL {url} -> {e}")


def _requests_get(url: str):
    if not _HAS_REQUESTS:
        raise RuntimeError("Missing 'requests' / 'beautifulsoup4' dependency")
    return requests.get(url, headers=REQ_HEADERS, timeout=REQ_TIMEOUT)


def _requests_html(url: str) -> str:
    """
    è¯·æ±‚ç½‘é¡µå¹¶è¿”å›è§£ç åçš„ HTML æ–‡æœ¬ï¼ˆå¤„ç† gbk/gb2312 ç­‰ï¼‰
    """
    resp = _requests_get(url)
    # ä¼˜å…ˆç”¨ apparent_encodingï¼ˆä¾èµ– chardetï¼‰ï¼Œå†é€€å› response.encoding
    enc = resp.apparent_encoding or resp.encoding or "utf-8"
    resp.encoding = enc
    return resp.text

def _extract_eastmoney_main(html: str) -> str:
    """
    ä¸œæ–¹è´¢å¯Œç«™ç‚¹ï¼ˆfinance.eastmoney.com ç­‰ï¼‰çš„å®šåˆ¶æŠ½å–ï¼š
    - ä¼˜å…ˆé”å®šæ­£æ–‡å®¹å™¨
    - ä»…æ”¶é›†æ­£æ–‡æ®µè½ï¼Œå¹¶è¿‡æ»¤â€œå…è´£å£°æ˜/ç‰ˆæƒå°¾å·´/å·¥å…·æ¡â€
    """
    soup = BeautifulSoup(html, "lxml")
    # å¸¸è§å®¹å™¨å€™é€‰ï¼ˆä¸œè´¢ä¸åŒé¢‘é“ class/id ä¼šå˜ï¼Œæ‰€ä»¥å¤šå‡†å¤‡å‡ ä¸ªï¼‰
    containers = [
        "#ContentBody", "#newsContent", ".newsContent", ".articleBody", ".article", ".content", ".media_article",
        ".main-content", ".m-article", ".g-article", ".detail"
    ]
    candidates = []
    for sel in containers:
        node = soup.select_one(sel)
        if not node:
            continue
        paras = [tag.get_text(" ", strip=True) for tag in node.find_all(["p","div","section","span"], recursive=True)]
        t = _clean_join_paragraphs(paras)
        if len(t) >= 60:
            candidates.append(t)

    # å¦‚æœä¸Šè¿°å®¹å™¨éƒ½ä¸é è°±ï¼Œé€€åŒ–ä¸ºâ€œä»é¡µé¢æ‰€æœ‰æ®µè½ä¸­é€‰å¯†åº¦æœ€é«˜çš„è‹¥å¹²æ®µâ€
    if not candidates:
        blocks = []
        for tag in soup.find_all(["p","div","section"]):
            txt = tag.get_text(" ", strip=True)
            if not txt or len(txt) < 15:
                continue
            blocks.append(txt)
        blocks.sort(key=lambda x: len(x), reverse=True)
        t = _clean_join_paragraphs(blocks[:20])
        if len(t) >= 60:
            candidates.append(t)

    if not candidates:
        return ""
    # é€‰åˆ†æ•°æœ€é«˜çš„å€™é€‰
    candidates.sort(key=_score_candidate_text, reverse=True)
    return candidates[0]

def _extract_with_boilerpy3(html: str) -> str:
    """
    boilerpy3 æŠ½å–æ­£æ–‡ï¼ˆå¯¹ä¸­æ–‡ä¹Ÿå‹å¥½ï¼‰
    """
    if not _HAS_BOILERPY3 or not html:
        return ""
    try:
        extractor = _bp_extractors.ArticleExtractor()
        text = extractor.get_content(html) or ""
        return text.strip()
    except Exception:
        return ""


def _extract_with_goose3(url: str, html: str) -> str:
    """
    goose3 æŠ½å–æ­£æ–‡ï¼šä¼˜å…ˆç”¨ raw_htmlï¼Œé¿å…å†æ¬¡å‘èµ·ç½‘ç»œè¯·æ±‚
    """
    if not _HAS_GOOSE3 or not html:
        return ""
    try:
        g = Goose({
            "enable_image_fetching": False,
            "use_meta_language": False,
            "target_language": "zh",
            "browser_user_agent": REQ_HEADERS.get("User-Agent", ""),
        })
        art = g.extract(raw_html=html)
        text = (art.cleaned_text or art.meta_description or "") if art else ""
        return text.strip()
    except Exception:
        return ""
def _extract_with_bs4(url: str, html: str) -> str:
    """
    é€šç”¨ + ç«™ç‚¹ç‰¹åŒ–ï¼ˆä¼˜å…ˆä¸œè´¢ï¼‰ï¼Œè¿”å›å°½å¯èƒ½æ¥è¿‘æ­£æ–‡çš„æ–‡æœ¬ã€‚
    """
    try:
        host = ""
        m = re.search(r"https?://([^/]+)/", url)
        if m:
            host = m.group(1).lower()
    except Exception:
        host = ""

    # â€”â€” ä¸œæ–¹è´¢å¯Œå¼ºå®šåˆ¶ â€”â€” 
    if "eastmoney.com" in host:
        t = _extract_eastmoney_main(html)
        if len(t) >= MIN_ARTICLE_CHARS:
            return t

    # â€”â€” é€šç”¨å®¹å™¨ â€”â€” 
    soup = BeautifulSoup(html, "lxml")
    common_candidates = [
        "article", ".article", ".article-content", ".content", ".post", "#article",
        ".main-content", ".detail", ".news_content", "#content", ".rich_media_content"
    ]
    texts = []
    for sel in common_candidates:
        node = soup.select_one(sel)
        if not node:
            continue
        paras = [tag.get_text(" ", strip=True) for tag in node.find_all(["p","div","section","span"], recursive=True)]
        t = _clean_join_paragraphs(paras)
        if len(t) >= 60:
            texts.append(t)

    # é€€åŒ–ï¼šå¤§å—æ‹¼æ¥
    if not texts:
        blocks = []
        for tag in soup.find_all(["p","div","section"]):
            txt = tag.get_text(" ", strip=True)
            if not txt or len(txt) < 15:
                continue
            blocks.append(txt)
        blocks.sort(key=lambda x: len(x), reverse=True)
        t = _clean_join_paragraphs(blocks[:20])
        if len(t) >= 60:
            texts.append(t)

    if not texts:
        return ""
    texts.sort(key=_score_candidate_text, reverse=True)
    return texts[0]



def _extract_with_readability(url: str, html: str) -> str:
    if not _HAS_READABILITY:
        return ""
    try:
        doc = Document(html)
        summary_html = doc.summary()  # æå–åçš„ HTML
        soup = BeautifulSoup(summary_html, "lxml")
        text = soup.get_text(separator=" ", strip=True)
        return text
    except Exception as e:
        _log_fail(f"READABILITY_FAIL {url} -> {e}")
        return ""


def _extract_with_newspaper(url: str) -> str:
    if not _HAS_NEWSPAPER:
        return ""
    try:
        art = Article(url, language="zh")
        art.download()
        art.parse()
        return (art.text or "").strip()
    except Exception as e:
        _log_fail(f"NEWSPAPER_FAIL {url} -> {e}")
        return ""

def _maybe_fetch_amp_html(url: str, html: str) -> str:
    """
    å¦‚é¡µé¢æä¾› <link rel="amphtml" href="...">ï¼ŒæŠ“å– AMP ç‰ˆæœ¬å†å°è¯•æŠ½å–ã€‚
    """
    try:
        soup = BeautifulSoup(html, "lxml")
        amp = soup.find("link", rel=lambda v: v and "amphtml" in v.lower())
        if amp and amp.get("href"):
            amp_url = amp["href"]
            if amp_url.startswith("//"):
                amp_url = "https:" + amp_url
            if amp_url.startswith("/"):
                # ç›¸å¯¹è·¯å¾„æƒ…å†µ
                base = re.match(r"(https?://[^/]+)", url)
                if base:
                    amp_url = base.group(1) + amp_url
            amp_html = _requests_html(amp_url)
            return amp_html
    except Exception as e:
        _log_fail(f"AMP_FETCH_FAIL {url} -> {e}")
    return ""
def fetch_article_text(url: str, cache_key: str = None) -> str:
    """
    è·å–æ–‡ç« å®Œæ•´æ­£æ–‡ï¼ˆå¸¦ç¼“å­˜ï¼‰ï¼š
    - å‘½ä¸­ç¼“å­˜ï¼ˆè‹¥ç»™äº† cache_key åˆ™ç”¨ {code}_{pubtime}.json å‘½åï¼‰
    - æŠ½å–é€»è¾‘ï¼šnewspaper3k â†’ readability â†’ bs4ï¼ˆç«™ç‚¹ç‰¹åŒ–+é€šç”¨ï¼‰â†’ AMP å›é€€ â†’ trafilatura
    - å¤šå€™é€‰æ‰“åˆ†æ‹©ä¼˜ + å…è´£å£°æ˜å»é™¤
    """
    if not url or not isinstance(url, str) or not url.startswith("http"):
        return ""

    # 1) ç¼“å­˜
    cached = _read_article_cache(url, cache_key)
    if cached:
        return cached

    candidates = []

    # 2) newspaper3k
    if _HAS_NEWSPAPER:
        try:
            t = _retry(lambda: _extract_with_newspaper(url))
            t = _strip_disclaimer_tail(_sanitize_text(t))
            if len(t) >= MIN_ARTICLE_CHARS:
                candidates.append(("newspaper", t))
        except Exception as e:
            _log_fail(f"NEWSPAPER_FETCH_FAIL {url} -> {e}")

    # 3) requests + readability / bs4
    html = ""
    try:
        html = _retry(lambda: _requests_html(url))
    except Exception as e:
        _log_fail(f"REQUESTS_FETCH_FAIL {url} -> {e}")

    if html:
        # 3a) readability
        if _HAS_READABILITY:
            try:
                t = _extract_with_readability(url, html)
                t = _strip_disclaimer_tail(_sanitize_text(t))
                if len(t) >= MIN_ARTICLE_CHARS:
                    candidates.append(("readability", t))
            except Exception:
                pass

        # 3b) bs4ï¼ˆå« eastmoney ç‰¹åŒ–ï¼‰
        try:
            t = _extract_with_bs4(url, html)
            t = _strip_disclaimer_tail(_sanitize_text(t))
            if len(t) >= MIN_ARTICLE_CHARS:
                candidates.append(("bs4", t))
        except Exception as e:
            _log_fail(f"BS4_EXTRACT_FAIL {url} -> {e}")
        
        # â€”â€” boilerpy3 å€™é€‰ â€”â€” 
        try:
            t = _extract_with_boilerpy3(html)
            t = _strip_disclaimer_tail(_sanitize_text(t))
            if len(t) >= MIN_ARTICLE_CHARS:
                candidates.append(("boilerpy3", t))
        except Exception as e:
            _log_fail(f"BOILERPY3_FAIL {url} -> {e}")

        # â€”â€” goose3 å€™é€‰ â€”â€” 
        try:
            t = _extract_with_goose3(url, html)
            t = _strip_disclaimer_tail(_sanitize_text(t))
            if len(t) >= MIN_ARTICLE_CHARS:
                candidates.append(("goose3", t))
        except Exception as e:
            _log_fail(f"GOOSE3_FAIL {url} -> {e}")

        # 3c) AMP å›é€€å†è¯•
        try:
            amp_html = _maybe_fetch_amp_html(url, html)
            if amp_html:
                # å…ˆ readability
                if _HAS_READABILITY:
                    t = _extract_with_readability(url, amp_html)
                    t = _strip_disclaimer_tail(_sanitize_text(t))
                    if len(t) >= MIN_ARTICLE_CHARS:
                        candidates.append(("amp_readability", t))
                # å† bs4
                t = _extract_with_bs4(url, amp_html)
                t = _strip_disclaimer_tail(_sanitize_text(t))
                if len(t) >= MIN_ARTICLE_CHARS:
                    candidates.append(("amp_bs4", t))
                # AMP + boilerpy3
                try:
                    t = _extract_with_boilerpy3(amp_html)
                    t = _strip_disclaimer_tail(_sanitize_text(t))
                    if len(t) >= MIN_ARTICLE_CHARS:
                        candidates.append(("amp_boilerpy3", t))
                except Exception as e:
                    _log_fail(f"AMP_BOILERPY3_FAIL {url} -> {e}")

                # AMP + goose3
                try:
                    t = _extract_with_goose3(url, amp_html)
                    t = _strip_disclaimer_tail(_sanitize_text(t))
                    if len(t) >= MIN_ARTICLE_CHARS:
                        candidates.append(("amp_goose3", t))
                except Exception as e:
                    _log_fail(f"AMP_GOOSE3_FAIL {url} -> {e}")
        except Exception as e:
            _log_fail(f"AMP_EXTRACT_FAIL {url} -> {e}")

    # 4) trafilatura å…œåº•
    if (not candidates) and _HAS_TRAFILATURA and html:
        try:
            t = trafilatura.extract(html, include_comments=False, favor_recall=True, output_format="txt")
            t = _strip_disclaimer_tail(_sanitize_text(t or ""))
            if len(t) >= MIN_ARTICLE_CHARS:
                candidates.append(("trafilatura", t))
        except Exception as e:
            _log_fail(f"TRAFILATURA_FAIL {url} -> {e}")

    # 5) é€‰æ‹©æœ€ä½³å€™é€‰
    best = ""
    if candidates:
        # æ ¹æ® _score_candidate_text æ‰“åˆ†æ‹©ä¼˜
        best = max(candidates, key=lambda kv: _score_candidate_text(kv[1]))[1]

    # 6) æˆªæ–­ + å†™ç¼“å­˜
    if best and len(best) > MAX_ARTICLE_CHARS:
        best = best[:MAX_ARTICLE_CHARS]
    if best:
        _write_article_cache(url, best, cache_key)

    return best or ""



# =================== æ–°é—»æƒ…ç»ªå› å­ ===================
def get_news_AKSHARE_STOCK_NEWS_EM(code: str, name: str = None, topk: int = NEWS_TOPK) -> float:
    """
    è·å–ä¸ªè‚¡æœ€è¿‘æ–°é—»â€œå®Œæ•´æ­£æ–‡â€çš„æƒ…æ„Ÿå¹³å‡å€¼ï¼ˆ[-1, 1]ï¼‰ã€‚
    - symbol ä¼˜å…ˆç”¨è‚¡ç¥¨åç§°ï¼ˆè‹¥ä¼ å…¥ nameï¼‰ï¼Œå¦åˆ™ç”¨ code
    - æ­£æ–‡ç¼“å­˜å‘½åï¼š{code}_{pub_time}
    - SnowNLP sentiments ä¿®æ­£ + ç®€å•å»é‡
    """
    path = os.path.join(AKSHARE_STOCK_NEWS_EM_DIR, f"{code}.csv")

    def fetch():
        symbol_str = name if (name and isinstance(name, str) and name.strip()) else code
        return ak.stock_news_em(symbol=symbol_str)

    # è¯»å–æˆ–åˆ·æ–°ç¼“å­˜
    df = None
    try:
        if _need_refresh(path, MAX_AGE_HOURS_NEWS):
            df = _retry(fetch)
            if isinstance(df, pd.DataFrame) and not df.empty:
                df.to_csv(path, index=False, encoding="utf-8-sig")
        else:
            df = pd.read_csv(path)
    except Exception as e:
        _log_fail(f"NEWS_FETCH_FAIL {code} -> {e}")
        if os.path.exists(path):
            try:
                df = pd.read_csv(path)
            except Exception:
                df = None

    if df is None or df.empty:
        return 0.0

    # åˆ—åé²æ£’ï¼šæ ‡é¢˜/å†…å®¹/é“¾æ¥/æ—¶é—´
    title_col = _get_first_existing_col(df, ["æ–°é—»æ ‡é¢˜", "æ ‡é¢˜", "news_title", "title"])
    content_col = _get_first_existing_col(df, ["æ–°é—»å†…å®¹", "æ‘˜è¦", "content", "desc"])
    link_col = _get_first_existing_col(df, ["æ–°é—»é“¾æ¥", "é“¾æ¥", "url", "link"])
    time_col = _get_first_existing_col(df, ["æ—¥æœŸ", "æ—¶é—´", "å‘å¸ƒæ—¶é—´", "time", "pub_time"])

    # æ—¶é—´åˆ—è½¬ datetime å¹¶å‡åº
    df = _to_datetime_sorted(df)
    if time_col in df.columns:
        try:
            df[time_col] = pd.to_datetime(df[time_col], errors="coerce")
        except Exception:
            pass

    # å–æœ€è¿‘ topk
    df_recent = df.tail(topk) if len(df) > topk else df

    # ç®€å•å»é‡ï¼šä¼˜å…ˆæŒ‰é“¾æ¥å»é‡ï¼›æ²¡æœ‰é“¾æ¥åˆ™æŒ‰ [æ ‡é¢˜, æ—¶é—´] å»é‡
    if link_col and link_col in df_recent.columns:
        df_recent = df_recent.drop_duplicates(subset=[link_col])
    else:
        keys = [c for c in [title_col, time_col] if c and c in df_recent.columns]
        if keys:
            df_recent = df_recent.drop_duplicates(subset=keys)

    texts = []
    for _, row in df_recent.iterrows():
        full_text = ""
        pub_key = None

        # ç”Ÿæˆç¼“å­˜ keyï¼š{code}_{å‘å¸ƒæ—¶é—´}
        if time_col and pd.notna(row.get(time_col, None)):
            pub_key = f"{code}_{_format_pub_time(row[time_col])}"
        else:
            pub_key = f"{code}_unknown"

        # 1) ä¼˜å…ˆæŠ“é“¾æ¥æ­£æ–‡ï¼ˆå¸¦ cache_key å‘½åï¼‰
        if link_col and pd.notna(row.get(link_col, None)):
            url = str(row[link_col]).strip()
            if url.startswith("http"):
                try:
                    full_text = fetch_article_text(url, cache_key=pub_key)
                except Exception as e:
                    _log_fail(f"FETCH_ARTICLE_FAIL {code} {url} -> {e}")

        # 2) å›é€€æ‘˜è¦/æ ‡é¢˜
        if not full_text or len(full_text) < MIN_ARTICLE_CHARS:
            fallback = None
            if content_col and pd.notna(row.get(content_col, None)):
                fallback = str(row[content_col])
            elif title_col and pd.notna(row.get(title_col, None)):
                fallback = str(row[title_col])
            if fallback:
                full_text = fallback

        if full_text:
            texts.append(full_text)

    if not texts:
        return 0.0

    # SnowNLP æƒ…æ„Ÿï¼šä¿®æ­£ä¸º .sentiments âˆˆ [0,1]
    scores = []
    for t in texts:
        try:
            s = SnowNLP(t).sentiments
            scores.append(s)
        except Exception as e:
            _log_fail(f"SNOWNLP_FAIL {code} -> {e}")

    if not scores:
        return 0.0

    mapped = [(s - 0.5) * 2 for s in scores]
    return round(float(pd.Series(mapped).mean()), 4)


# =================== æ‰¹é‡æå–ï¼ˆå¹¶å‘ + å¯æ‰©å±•ï¼‰ ===================
def _one_code(code: str, name: str = None) -> dict:
    # è·å–Akshareä¸ªè‚¡æ–°é—»ï¼ˆä¸œè´¢æ¥å£ï¼‰
    try:
        senti_akshre_em = get_news_AKSHARE_STOCK_NEWS_EM(code, name=name, topk=NEWS_TOPK)
    except Exception as e:
        _log_fail(f"AKSHARE_STOCK_NEWS_EM_FAIL {code} -> {e}")
        senti_akshre_em = 0.0
        
    # å…¶ä»–æ–°é—»æ¨¡å—
    
    
    # å…¶ä»–æ¶ˆæ¯æ¨¡å— 

    _random_pause()

    return {
        "ä»£ç ": code,
        "AKSHAREä¸œè´¢æ¥å£çš„æ–°é—»æƒ…ç»ª": senti_akshre_em,
        # å…¶ä»–ç»“æœè¿”å›
    }


def extract_AKSHARE_STOCK_NEWS_EM_fund_features(code_list, max_workers: int = MAX_WORKERS) -> pd.DataFrame:
    """
    code_list æ”¯æŒä¸¤ç§å…ƒç´ å½¢æ€ï¼š
    - "000001"ï¼ˆåªæœ‰ä»£ç ï¼‰
    - ("000001", "å¹³å®‰é“¶è¡Œ")ï¼ˆä»£ç  + åç§°ï¼‰
    """
    rows = []
    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        futs = {}
        for item in code_list:
            if isinstance(item, (list, tuple)) and len(item) >= 2:
                code, name = item[0], item[1]
            else:
                code, name = item, None
            fut = ex.submit(_one_code, code, name)
            futs[fut] = code

        for fut in tqdm(as_completed(futs), total=len(futs), desc="Extracting"):
            try:
                rows.append(fut.result())
            except Exception as e:
                code = futs.get(fut, "UNKNOWN")
                _log_fail(f"TASK_FAIL {code} -> {e}")

    df = pd.DataFrame(rows)
    df.insert(0, "ç”Ÿæˆæ—¶é—´", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    df.insert(1, "æ–°é—»topk", NEWS_TOPK)
    # df.insert(2, "èµ„é‡‘å¤©æ•°", FUND_DAYS)
    # df.insert(3, "é¾™è™æ¦œå¤©æ•°", LHB_DAYS)
    return df


# =================== ä¸»ç¨‹åº ===================
if __name__ == '__main__':
    # ç¤ºä¾‹ï¼šè·‘ä¸€éè‚¡ç¥¨åˆ—è¡¨ï¼ˆéœ€åŒ…å«â€œä»£ç â€åˆ—ï¼›å»ºè®®å­—ç¬¦ä¸²ç±»å‹ä¿ç•™å‰å¯¼0ï¼‰
    stock_list_path = os.path.join(META_DIR, "stock_list.csv")
    if not os.path.exists(stock_list_path):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°è‚¡ç¥¨åˆ—è¡¨ï¼š{stock_list_path}ï¼Œè¯·å…ˆå‡†å¤‡åŒ…å«â€˜ä»£ç â€™åˆ—çš„CSVã€‚")

    df_list = pd.read_csv(stock_list_path, dtype=str)
    # è·å–è‚¡ç¥¨ä»£ç åˆ—
    if "ä»£ç " not in df_list.columns:
        # åšä¸€ä¸ªå…œåº•ï¼šå¸¸è§å­—æ®µå candidates
        cand = _get_first_existing_col(df_list, ["ä»£ç ", "symbol", "ts_code", "code"])
        if cand and cand != "ä»£ç ":
            df_list.rename(columns={cand: "ä»£ç "}, inplace=True)
        if "ä»£ç " not in df_list.columns:
            raise ValueError("è‚¡ç¥¨åˆ—è¡¨ç¼ºå°‘â€˜ä»£ç â€™åˆ—ï¼ˆæˆ–å¸¸è§ç­‰ä»·åˆ—ï¼‰ã€‚")

    codes = df_list["ä»£ç "].dropna().astype(str).unique().tolist()

    # è·å–è‚¡ç¥¨åç§°åˆ—ï¼ˆå¯é€‰ï¼‰ï¼š['åç§°','è‚¡ç¥¨åç§°','è‚¡ç¥¨ç®€ç§°','name']
    name_col = _get_first_existing_col(df_list, ["åç§°", "è‚¡ç¥¨åç§°", "è‚¡ç¥¨ç®€ç§°", "name"])
    if name_col and name_col in df_list.columns:
        # ç»„è£… (code, name) åˆ—è¡¨
        merged = (
            df_list.dropna(subset=["ä»£ç "])
                .astype({"ä»£ç ": str})
        )
        # æ¯ä¸ªä»£ç åªå–ç¬¬ä¸€æ¡åç§°
        merged = merged.drop_duplicates(subset=["ä»£ç "])
        code_name_list = list(zip(merged["ä»£ç "].tolist(), merged[name_col].astype(str).tolist()))
    else:
        code_name_list = codes  # æ²¡æœ‰åç§°åˆ—å°±åªç”¨ä»£ç 

    df_feat = extract_AKSHARE_STOCK_NEWS_EM_fund_features(code_name_list, max_workers=MAX_WORKERS)
    df_feat.to_csv(OUT_PATH, index=False, encoding="utf-8-sig")
    print(df_feat.head())
    print(f"\nâœ… å·²ä¿å­˜ï¼š{OUT_PATH}")
    print(f"ğŸ“„ å¤±è´¥æ—¥å¿—ï¼ˆå¦‚æœ‰ï¼‰ï¼š{FAIL_LOG}")
