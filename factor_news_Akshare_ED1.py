# -*- coding: utf-8 -*-
"""
ä¸ªè‚¡ æ¶ˆæ¯é¢ + èµ„é‡‘é¢ æ•°æ®æå–ï¼ˆå•æ–‡ä»¶ Â· æ­£æ–‡æŠ“å–å‡çº§ç‰ˆï¼‰
- æ–°é—»æ ‡é¢˜åˆ—åé²æ£’æ€§ + æ—¥æœŸæ’åº
- æ­£æ–‡æŠ“å–ï¼šä¼˜å…ˆç”¨â€œæ–°é—»é“¾æ¥â€è·å–å®Œæ•´æ–‡ç« å†…å®¹ï¼ˆå¤šç­–ç•¥ + ç«™ç‚¹ç‰¹åŒ–ï¼‰
- æ­£æ–‡/æ–°é—»ç¼“å­˜ï¼ˆé¿å…é‡å¤æŠ“ï¼‰
- ç¼“å­˜è¿‡æœŸåˆ·æ–°ï¼ˆæ–°é—»åˆ—è¡¨/èµ„é‡‘æµ/æ­£æ–‡ï¼‰
- é‡è¯• + æŒ‡æ•°é€€é¿ + å¤±è´¥æ—¥å¿—
- é¾™è™æ¦œï¼šflag & æ¬¡æ•°
- å¹¶å‘æé€Ÿ + è½»é‡é™é€Ÿ
"""

import os
import re
import time
import json
import hashlib
import random
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed

import pandas as pd
import akshare as ak
from snownlp import SnowNLP
from tqdm import tqdm

# â€”â€” å¯é€‰ä¾èµ–ï¼šè‡ªåŠ¨ä¼˜å…ˆä½¿ç”¨ï¼Œç¼ºå°‘åˆ™è‡ªåŠ¨é™çº§ â€”â€”
try:
    from newspaper import Article  # pip install newspaper3k
    _HAS_NEWSPAPER = True
except Exception:
    _HAS_NEWSPAPER = False

try:
    from readability import Document  # pip install readability-lxml
    _HAS_READABILITY = True
except Exception:
    _HAS_READABILITY = False

try:
    import requests
    from bs4 import BeautifulSoup  # pip install beautifulsoup4 lxml
    _HAS_REQUESTS = True
except Exception:
    _HAS_REQUESTS = False


# =================== å¯é…ç½®å‚æ•° ===================
R00T_DIR = "data"
META_DIR = "data/metadata"
STOCK_NEWS_DIR = os.path.join(R00T_DIR, "stock_news")
RESULT_DIR = os.path.join(STOCK_NEWS_DIR, "news_nlp_result_by_day")
os.makedirs(META_DIR, exist_ok=True)
os.makedirs(RESULT_DIR, exist_ok=True)

RUN_TS = datetime.now().strftime("%Y%m%d_%H%M%S")
OUT_PATH = os.path.join(RESULT_DIR, f"stock_news_factors_{RUN_TS}.csv") # æ–°é—»è¯„åˆ†ç»“æœè¾“å‡º(å¸¦æ—¥æœŸ)
FAIL_LOG = os.path.join(RESULT_DIR, f"stock_news_failures_{RUN_TS}.log") # æ–°é—»è·å–å¤±è´¥ç»“æœï¼ˆå¸¦æ—¥æœŸï¼‰

# Akshare ä¸œè´¢ä¸ªè‚¡æ–°é—»æ¥å£
AKSHARE_STOCK_NEWS_EM_DIR = os.path.join(STOCK_NEWS_DIR, "akshare_stock_news_em")
AKSHARE_STOCK_NEWS_EM_ARTICLES_DIR = os.path.join(AKSHARE_STOCK_NEWS_EM_DIR, "articles")     # æ­£æ–‡ç¼“å­˜ç›®å½•
os.makedirs(AKSHARE_STOCK_NEWS_EM_DIR, exist_ok=True)
os.makedirs(AKSHARE_STOCK_NEWS_EM_ARTICLES_DIR, exist_ok=True)



# å…¶ä»–æ–¹æ³•/æ¥å£ç›®å½•


# ç¼“å­˜æ–‡ä»¶æœ€å¤§æœ‰æ•ˆæœŸï¼ˆå°æ—¶ï¼‰ï¼Œè¶…è¿‡åˆ™åˆ·æ–°æŠ“å–
MAX_AGE_HOURS_NEWS = 24        # æ–°é—»åˆ—è¡¨ç¼“å­˜
MAX_AGE_HOURS_ARTICLE = 24*7   # æ­£æ–‡ç¼“å­˜ï¼šä¸€å‘¨

# æŠ“å–é‡è¯•
RETRIES = 3
BASE_WAIT = 0.8  # ç§’ï¼ŒæŒ‡æ•°é€€é¿åŸºæ•°

# å¹¶å‘çº¿ç¨‹æ•°
MAX_WORKERS = 8

# è½»é‡é™é€Ÿï¼šæ¯åªè‚¡ç¥¨æŠ“å®Œåéšæœº sleep åŒºé—´ï¼ˆç§’ï¼‰
RANDOM_SLEEP_RANGE = (0.05, 0.2)

# æ–°é—»æƒ…ç»ªï¼šå–â€œæœ€è¿‘ topk æ¡â€ç”¨äºæƒ…ç»ªæ‰“åˆ†
NEWS_TOPK = 20

# æ­£æ–‡æœ€å°é•¿åº¦é˜ˆå€¼ï¼ˆå¤ªçŸ­è¯´æ˜æŠ½å–å¤±è´¥ï¼Œå›é€€æ ‡é¢˜/æ‘˜è¦ï¼‰
MIN_ARTICLE_CHARS = 60
# å¯¹è¶…é•¿æ­£æ–‡å¯æˆªæ–­åšæƒ…æ„Ÿï¼ˆé¿å…è¶…æ…¢ï¼‰ï¼ŒæŒ‰å­—ç¬¦æˆªæ–­
MAX_ARTICLE_CHARS = 6000

# Requests æŠ“å–å‚æ•°
REQ_TIMEOUT = 12
REQ_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/124.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "zh-CN,zh;q=0.9",
}


# =================== å·¥å…·å‡½æ•° ===================
def _need_refresh(path: str, max_age_hours: int) -> bool:
    if not os.path.exists(path):
        return True
    try:
        mtime = datetime.fromtimestamp(os.path.getmtime(path))
        return (datetime.now() - mtime).total_seconds() > max_age_hours * 3600
    except Exception:
        return True


def _retry(fn, tries=RETRIES, base_wait=BASE_WAIT):
    last_err = None
    for i in range(tries):
        try:
            return fn()
        except Exception as e:
            last_err = e
            if i < tries - 1:
                time.sleep(base_wait * (2 ** i) + random.random() * 0.3)
    raise last_err


def _log_fail(msg: str):
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(FAIL_LOG, "a", encoding="utf-8") as f:
        f.write(f"[{ts}] {msg}\n")


def _random_pause():
    low, high = RANDOM_SLEEP_RANGE
    time.sleep(random.uniform(low, high))


def _get_first_existing_col(df: pd.DataFrame, candidates):
    for c in candidates:
        if c in df.columns:
            return c
    return None


def _to_datetime_sorted(df: pd.DataFrame, date_candidates=("æ—¥æœŸ", "æ—¶é—´", "å‘å¸ƒæ—¶é—´", "time", "pub_time")):
    """
    æ‰¾åˆ°ä¸€ä¸ªæ—¥æœŸåˆ—ï¼Œè½¬æ¢ä¸º datetime å¹¶æŒ‰å‡åºæ’åºï¼›è‹¥æ‰¾åˆ°å¤šåˆ—ï¼Œå–ç¬¬ä¸€ä¸ª
    """
    for c in date_candidates:
        if c in df.columns:
            try:
                df[c] = pd.to_datetime(df[c], errors="coerce")
                df = df.sort_values(c, ascending=True)  # å‡åºï¼Œä¾¿äº tail(N) å–æœ€è¿‘ N æ¡
                return df
            except Exception:
                continue
    return df  # æ‰¾ä¸åˆ°æ—¥æœŸåˆ—å°±åŸæ ·è¿”å›


def _md5(s: str) -> str:
    return hashlib.md5(s.encode("utf-8")).hexdigest()


def _sanitize_text(txt: str) -> str:
    if not txt:
        return ""
    # å»è„šæ³¨/é‡å¤ç©ºç™½
    txt = re.sub(r"\s+", " ", txt)
    # å¸¸è§ç‰ˆæƒå°¾å·´ç²—ç•¥å‰”é™¤ï¼ˆå¯æŒ‰éœ€æ‰©å±•ï¼‰
    txt = re.sub(r"(è´£ä»»ç¼–è¾‘[:ï¼š].*?$)", "", txt)
    return txt.strip()


# =================== æ­£æ–‡æŠ“å–ä¸ç¼“å­˜ ===================
def _article_cache_path(url: str) -> str:
    return os.path.join(AKSHARE_STOCK_NEWS_EM_ARTICLES_DIR, f"{_md5(url)}.json")


def _read_article_cache(url: str):
    path = _article_cache_path(url)
    if not os.path.exists(path):
        return None
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        # è¿‡æœŸåˆ¤æ–­
        ts = datetime.fromisoformat(data.get("fetched_at"))
        if (datetime.now() - ts).total_seconds() > MAX_AGE_HOURS_ARTICLE * 3600:
            return None
        return data.get("text", "")
    except Exception:
        return None


def _write_article_cache(url: str, text: str):
    path = _article_cache_path(url)
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump({"url": url, "text": text, "fetched_at": datetime.now().isoformat()}, f, ensure_ascii=False)
    except Exception as e:
        _log_fail(f"WRITE_ARTICLE_CACHE_FAIL {url} -> {e}")


def _requests_get(url: str):
    if not _HAS_REQUESTS:
        raise RuntimeError("Missing 'requests' / 'beautifulsoup4' dependency")
    return requests.get(url, headers=REQ_HEADERS, timeout=REQ_TIMEOUT)


def _requests_html(url: str) -> str:
    """
    è¯·æ±‚ç½‘é¡µå¹¶è¿”å›è§£ç åçš„ HTML æ–‡æœ¬ï¼ˆå¤„ç† gbk/gb2312 ç­‰ï¼‰
    """
    resp = _requests_get(url)
    # ä¼˜å…ˆç”¨ apparent_encodingï¼ˆä¾èµ– chardetï¼‰ï¼Œå†é€€å› response.encoding
    enc = resp.apparent_encoding or resp.encoding or "utf-8"
    resp.encoding = enc
    return resp.text


def _extract_with_bs4(url: str, html: str) -> str:
    """
    é€šç”¨ + ç«™ç‚¹ç‰¹åŒ–çš„æ­£æ–‡æŠ½å–ï¼ˆBeautifulSoupï¼‰
    """
    soup = BeautifulSoup(html, "lxml")

    # â€”â€” ç«™ç‚¹ç‰¹åŒ–ï¼šä¸œæ–¹è´¢å¯Œï¼ˆeastmoneyï¼‰å¸¸è§å®¹å™¨é€‰æ‹©å™¨ â€”â€” 
    # ä¸åŒé¢‘é“/å¹´ä»£ class/id å¯èƒ½ä¸åŒï¼Œå‡†å¤‡å¤šä¸ªå€™é€‰
    if "eastmoney.com" in url:
        candidates = [
            "#ContentBody",               # å¸¸è§å†…å®¹å®¹å™¨
            ".newsContent",               # æ—§ç‰ˆ
            ".content",                   # é€šç”¨
            ".articleBody", ".article", ".media_article", "#newsContent"
        ]
        for sel in candidates:
            node = soup.select_one(sel)
            if node:
                text = node.get_text(separator=" ", strip=True)
                if len(text) >= MIN_ARTICLE_CHARS:
                    return text

    # â€”â€” æ¯æ—¥ç»æµæ–°é—»ï¼ˆnbd.com.cnï¼‰ / åŒæºè½¬è½½åˆ° eastmoney â€”â€” 
    if "nbd.com.cn" in url:
        candidates = [".g-article", ".article", ".content", ".m-article"]
        for sel in candidates:
            node = soup.select_one(sel)
            if node:
                text = node.get_text(separator=" ", strip=True)
                if len(text) >= MIN_ARTICLE_CHARS:
                    return text

    # â€”â€” é€šç”¨ï¼šå°è¯•å¯»æ‰¾å¸¸è§æ­£æ–‡å®¹å™¨ â€”â€” 
    common_candidates = [
        "article", ".article", ".article-content", ".content", ".post", "#article",
        ".main-content", ".detail", ".news_content", "#content", ".rich_media_content"
    ]
    for sel in common_candidates:
        node = soup.select_one(sel)
        if node:
            text = node.get_text(separator=" ", strip=True)
            if len(text) >= MIN_ARTICLE_CHARS:
                return text

    # â€”â€” é€€åŒ–ï¼šå–æœ€å¤§æ–‡æœ¬å—ï¼ˆæ®µè½ç»¼åˆï¼‰ â€”â€” 
    blocks = [p.get_text(" ", strip=True) for p in soup.find_all(["p", "div", "section"]) if p]
    blocks = [b for b in blocks if b and len(b) > 10]
    blocks.sort(key=lambda x: len(x), reverse=True)
    text = " ".join(blocks[:15])  # å–è‹¥å¹²å¤§å—æ‹¼æ¥
    return text


def _extract_with_readability(url: str, html: str) -> str:
    if not _HAS_READABILITY:
        return ""
    try:
        doc = Document(html)
        summary_html = doc.summary()  # æå–åçš„ HTML
        soup = BeautifulSoup(summary_html, "lxml")
        text = soup.get_text(separator=" ", strip=True)
        return text
    except Exception as e:
        _log_fail(f"READABILITY_FAIL {url} -> {e}")
        return ""


def _extract_with_newspaper(url: str) -> str:
    if not _HAS_NEWSPAPER:
        return ""
    try:
        art = Article(url, language="zh")
        art.download()
        art.parse()
        return (art.text or "").strip()
    except Exception as e:
        _log_fail(f"NEWSPAPER_FAIL {url} -> {e}")
        return ""


def fetch_article_text(url: str) -> str:
    """
    è·å–æ–‡ç« å®Œæ•´æ­£æ–‡ï¼ˆå¸¦ç¼“å­˜ï¼‰ï¼Œæ­¥éª¤ï¼š
    1) å‘½ä¸­ç¼“å­˜è¿”å›
    2) newspaper3k
    3) requests + readability
    4) requests + BeautifulSoupï¼ˆç«™ç‚¹ç‰¹åŒ– + é€šç”¨ï¼‰
    """
    if not url or not isinstance(url, str) or not url.startswith("http"):
        return ""

    # å‘½ä¸­ç¼“å­˜
    cached = _read_article_cache(url)
    if cached:
        return cached

    text = ""

    # æ–¹æ¡ˆ 1ï¼šnewspaper3k
    if _HAS_NEWSPAPER:
        try:
            text = _retry(lambda: _extract_with_newspaper(url))
        except Exception as e:
            _log_fail(f"NEWSPAPER_FETCH_FAIL {url} -> {e}")

    # æ–¹æ¡ˆ 2/3ï¼šrequests + readability / bs4
    if not text or len(text) < MIN_ARTICLE_CHARS:
        if not _HAS_REQUESTS:
            _log_fail(f"REQUESTS_MISSING {url}")
        else:
            try:
                html = _retry(lambda: _requests_html(url))
                # 2) readability
                if _HAS_READABILITY:
                    try:
                        text = _extract_with_readability(url, html)
                    except Exception:
                        pass
                # 3) BeautifulSoupï¼ˆç«™ç‚¹ç‰¹åŒ– + é€šç”¨ï¼‰
                if not text or len(text) < MIN_ARTICLE_CHARS:
                    text = _extract_with_bs4(url, html)
            except Exception as e:
                _log_fail(f"REQUESTS_FETCH_FAIL {url} -> {e}")

    # æ¸…æ´— + æˆªæ–­
    text = _sanitize_text(text)
    if len(text) > MAX_ARTICLE_CHARS:
        text = text[:MAX_ARTICLE_CHARS]

    # å†™ç¼“å­˜ï¼ˆå³ä½¿è¾ƒçŸ­ä¹Ÿå†™å…¥ï¼Œé¿å…é¢‘ç¹è¯·æ±‚ï¼‰
    if text:
        _write_article_cache(url, text)

    return text


# =================== æ–°é—»æƒ…ç»ªå› å­ ===================
def get_news_AKSHARE_STOCK_NEWS_EM(code: str, topk: int = NEWS_TOPK) -> float:
    """
    è·å–ä¸ªè‚¡æœ€è¿‘æ–°é—»â€œå®Œæ•´æ­£æ–‡â€çš„æƒ…æ„Ÿå¹³å‡å€¼ï¼ˆ[-1, 1]ï¼‰ï¼ŒæŠ“ä¸åˆ°æ­£æ–‡åˆ™å›é€€æ ‡é¢˜/æ‘˜è¦ã€‚
    """
    path = os.path.join(AKSHARE_STOCK_NEWS_EM_DIR, f"{code}.csv")

    def fetch():
        return ak.stock_news_em(symbol=code)

    # è¯»å–æˆ–åˆ·æ–°ç¼“å­˜
    df = None
    try:
        if _need_refresh(path, MAX_AGE_HOURS_NEWS):
            df = _retry(fetch)
            if isinstance(df, pd.DataFrame) and not df.empty:
                df.to_csv(path, index=False, encoding="utf-8-sig")
        else:
            df = pd.read_csv(path)
    except Exception as e:
        _log_fail(f"NEWS_FETCH_FAIL {code} -> {e}")
        # å¤±è´¥å°±å°è¯•ç”¨æ—§ç¼“å­˜
        if os.path.exists(path):
            try:
                df = pd.read_csv(path)
            except Exception:
                df = None

    if df is None or df.empty:
        return 0.0

    # æ ‡é¢˜/å†…å®¹/é“¾æ¥åˆ—
    title_col = _get_first_existing_col(df, ["æ–°é—»æ ‡é¢˜", "æ ‡é¢˜", "news_title", "title"])
    content_col = _get_first_existing_col(df, ["æ–°é—»å†…å®¹", "æ‘˜è¦", "content", "desc"])
    link_col = _get_first_existing_col(df, ["æ–°é—»é“¾æ¥", "é“¾æ¥", "url", "link"])
    # æ—¥æœŸæ’åº
    df = _to_datetime_sorted(df)

    # å–æœ€è¿‘ topk æ¡
    df_recent = df.tail(topk) if len(df) > topk else df

    texts = []
    for _, row in df_recent.iterrows():
        full_text = ""
        # 1) ä¼˜å…ˆæŠ“å–æ–°é—»é“¾æ¥çš„å®Œæ•´æ­£æ–‡
        if link_col and pd.notna(row.get(link_col, None)):
            url = str(row[link_col]).strip()
            if url.startswith("http"):
                try:
                    full_text = fetch_article_text(url)
                except Exception as e:
                    _log_fail(f"FETCH_ARTICLE_FAIL {code} {url} -> {e}")

        # 2) æŠ“ä¸åˆ°æ­£æ–‡åˆ™å›é€€åˆ°â€œæ–°é—»å†…å®¹/æ‘˜è¦/æ ‡é¢˜â€
        if not full_text or len(full_text) < MIN_ARTICLE_CHARS:
            fallback = None
            if content_col and pd.notna(row.get(content_col, None)):
                fallback = str(row[content_col])
            elif title_col and pd.notna(row.get(title_col, None)):
                fallback = str(row[title_col])
            if fallback:
                full_text = fallback

        if full_text:
            texts.append(full_text)

    if not texts:
        return 0.0

    # SnowNLP æƒ…æ„Ÿï¼šå¯¹æ¯ç¯‡å–åˆ†ï¼Œå†å¹³å‡
    scores = []
    for t in texts:
        try:
            s = SnowNLP(t).sentiments  # [0,1]
            scores.append(s)
        except Exception as e:
            _log_fail(f"SNOWNLP_FAIL {code} -> {e}")
            continue

    if not scores:
        return 0.0

    # æ˜ å°„åˆ° [-1, 1] åå–å‡å€¼
    mapped = [(s - 0.5) * 2 for s in scores]
    return round(float(pd.Series(mapped).mean()), 4)


# =================== æ‰¹é‡æå–ï¼ˆå¹¶å‘ + å¯æ‰©å±•ï¼‰ ===================
def _one_code(code: str) -> dict:
    # è·å–Akshareä¸ªè‚¡æ–°é—»ï¼ˆä¸œè´¢æ¥å£ï¼‰
    try:
        senti = get_news_AKSHARE_STOCK_NEWS_EM(code, topk=NEWS_TOPK)
    except Exception as e:
        _log_fail(f"AKSHARE_STOCK_NEWS_EM_FAIL {code} -> {e}")
        senti = 0.0
        
    # å…¶ä»–æ–°é—»æ¨¡å—
    
    
    # å…¶ä»–æ¶ˆæ¯æ¨¡å— 

    _random_pause()

    return {
        "ä»£ç ": code,
        "æ–°é—»æƒ…ç»ª": senti,
        # å…¶ä»–ç»“æœè¿”å›
    }


def extract_AKSHARE_STOCK_NEWS_EM_fund_features(code_list, max_workers: int = MAX_WORKERS) -> pd.DataFrame:
    rows = []
    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        futs = {ex.submit(_one_code, code): code for code in code_list}
        for fut in tqdm(as_completed(futs), total=len(futs), desc="Extracting"):
            try:
                rows.append(fut.result())
            except Exception as e:
                code = "UNKNOWN"
                try:
                    code = futs[fut]
                except Exception:
                    pass
                _log_fail(f"TASK_FAIL {code} -> {e}")
    df = pd.DataFrame(rows)
    # é™„åŠ å…ƒæ•°æ®åˆ—ï¼ˆå¯é€‰ï¼‰
    df.insert(0, "ç”Ÿæˆæ—¶é—´", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    df.insert(1, "æ–°é—»topk", NEWS_TOPK)
    # df.insert(2, "èµ„é‡‘å¤©æ•°", FUND_DAYS)
    # df.insert(3, "é¾™è™æ¦œå¤©æ•°", LHB_DAYS)
    return df


# =================== ä¸»ç¨‹åº ===================
if __name__ == '__main__':
    # ç¤ºä¾‹ï¼šè·‘ä¸€éè‚¡ç¥¨åˆ—è¡¨ï¼ˆéœ€åŒ…å«â€œä»£ç â€åˆ—ï¼›å»ºè®®å­—ç¬¦ä¸²ç±»å‹ä¿ç•™å‰å¯¼0ï¼‰
    stock_list_path = os.path.join(META_DIR, "stock_list.csv")
    if not os.path.exists(stock_list_path):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°è‚¡ç¥¨åˆ—è¡¨ï¼š{stock_list_path}ï¼Œè¯·å…ˆå‡†å¤‡åŒ…å«â€˜ä»£ç â€™åˆ—çš„CSVã€‚")

    df_list = pd.read_csv(stock_list_path, dtype=str)
    if "ä»£ç " not in df_list.columns:
        # åšä¸€ä¸ªå…œåº•ï¼šå¸¸è§å­—æ®µå candidates
        cand = _get_first_existing_col(df_list, ["ä»£ç ", "symbol", "ts_code", "code"])
        if cand and cand != "ä»£ç ":
            df_list.rename(columns={cand: "ä»£ç "}, inplace=True)
        if "ä»£ç " not in df_list.columns:
            raise ValueError("è‚¡ç¥¨åˆ—è¡¨ç¼ºå°‘â€˜ä»£ç â€™åˆ—ï¼ˆæˆ–å¸¸è§ç­‰ä»·åˆ—ï¼‰ã€‚")

    codes = df_list["ä»£ç "].dropna().astype(str).unique().tolist()
    df_feat = extract_AKSHARE_STOCK_NEWS_EM_fund_features(codes, max_workers=MAX_WORKERS)
    df_feat.to_csv(OUT_PATH, index=False, encoding="utf-8-sig")
    print(df_feat.head())
    print(f"\nâœ… å·²ä¿å­˜ï¼š{OUT_PATH}")
    print(f"ğŸ“„ å¤±è´¥æ—¥å¿—ï¼ˆå¦‚æœ‰ï¼‰ï¼š{FAIL_LOG}")
