# è°ƒç”¨akshareçš„ä¸ªè‚¡ç ”æŠ¥æ¥å£å¹¶è¿›è¡Œnlpåˆ†æ
import os
import re
import time
import json
import hashlib
import random
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed

import pandas as pd
import akshare as ak
from snownlp import SnowNLP
from tqdm import tqdm

from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

try:
    from curl_cffi import requests as cf_requests
    _HAS_CURL_CFFI = True
except Exception:
    _HAS_CURL_CFFI = False

try:
    import tls_client
    _HAS_TLS_CLIENT = True
except Exception:
    _HAS_TLS_CLIENT = False

try:
    import requests
    from bs4 import BeautifulSoup  # pip install beautifulsoup4 lxml
    _HAS_REQUESTS = True
except Exception:
    _HAS_REQUESTS = False

# â€”â€” å¯é€‰ä¾èµ–ï¼šPDF æ­£æ–‡æŠ½å–ï¼ˆå¤šè·¯å…œåº•ï¼‰ â€”â€”
try:
    from pdfminer.high_level import extract_text as _pdfminer_extract_text  # pip install pdfminer.six -i https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple 
    _HAS_PDFMINER = True
except Exception:
    _HAS_PDFMINER = False

try:
    import fitz  # PyMuPDF
    _HAS_PYMUPDF = True
except Exception:
    _HAS_PYMUPDF = False

R00T_DIR = "data"
META_DIR = "data/metadata"
STOCK_NEWS_DIR = os.path.join(R00T_DIR, "stock_news")
RESULT_DIR = os.path.join(STOCK_NEWS_DIR, "reports_nlp_result_by_day")
os.makedirs(META_DIR, exist_ok=True)
os.makedirs(RESULT_DIR, exist_ok=True)

# â€”â€” ç ”æŠ¥ï¼ˆä¸ªè‚¡ç ”æŠ¥ï¼‰ç¼“å­˜ç›®å½• â€”â€”
AKSHARE_STOCK_REPORT_EM_DIR = os.path.join(STOCK_NEWS_DIR, "akshare_stock_report_em")
AKSHARE_STOCK_REPORT_EM_TEXT_DIR = os.path.join(AKSHARE_STOCK_REPORT_EM_DIR, "reports")
RUN_TS = datetime.now().strftime("%Y%m%d_%H%M%S")
OUT_PATH = os.path.join(RESULT_DIR, f"stock_reports_factors_{RUN_TS}.csv") # æ–°é—»è¯„åˆ†ç»“æœè¾“å‡º(å¸¦æ—¥æœŸ)
FAIL_LOG = os.path.join(RESULT_DIR, f"stock_reports_failures_{RUN_TS}.log") # æ–°é—»è·å–å¤±è´¥ç»“æœï¼ˆå¸¦æ—¥æœŸï¼‰

os.makedirs(AKSHARE_STOCK_REPORT_EM_DIR, exist_ok=True)
os.makedirs(AKSHARE_STOCK_REPORT_EM_TEXT_DIR, exist_ok=True)

# ä»…åˆ†æè¿‘ N å¹´ç ”æŠ¥
REPORT_WINDOW_YEARS = 2

# å¹¶å‘çº¿ç¨‹æ•°
MAX_WORKERS = 8

# è½»é‡é™é€Ÿï¼šæ¯åªè‚¡ç¥¨æŠ“å®Œåéšæœº sleep åŒºé—´ï¼ˆç§’ï¼‰
RANDOM_SLEEP_RANGE = (0.05, 0.2)

# Requests æŠ“å–å‚æ•°
REQ_TIMEOUT = 12
REQ_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/124.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "zh-CN,zh;q=0.9",
}

# ç¼“å­˜æ–‡ä»¶æœ€å¤§æœ‰æ•ˆæœŸï¼ˆå°æ—¶ï¼‰ï¼Œè¶…è¿‡åˆ™åˆ·æ–°æŠ“å–
MAX_AGE_HOURS_NEWS = 24        # æ–°é—»åˆ—è¡¨ç¼“å­˜
MAX_AGE_HOURS_REPORT = 24 * 7  # ç ”æŠ¥æ–‡æœ¬ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆä¸€å‘¨ï¼‰
REPORT_TOPK = 10               # æœ€è¿‘ topk ç¯‡ç ”æŠ¥ç”¨äºæ‰“åˆ†
REPORT_TEXT_MAX_CHARS = 12000  # è§£æ PDF åç”¨äºæƒ…æ„Ÿçš„æœ€å¤§å­—ç¬¦æ•°

# æŠ“å–é‡è¯•
RETRIES = 3
BASE_WAIT = 0.8  # ç§’ï¼ŒæŒ‡æ•°é€€é¿åŸºæ•°

# ç ”æŠ¥æ¡æ•°ï¼šå–â€œæœ€è¿‘ topk æ¡â€ç”¨äºæƒ…ç»ªæ‰“åˆ†
REPORTS_TOPK = 20
# æ­£æ–‡æœ€å°é•¿åº¦é˜ˆå€¼ï¼ˆå¤ªçŸ­è¯´æ˜æŠ½å–å¤±è´¥ï¼Œå›é€€æ ‡é¢˜/æ‘˜è¦ï¼‰
MIN_ARTICLE_CHARS = 150

# â€œä¸œè´¢è¯„çº§â€æ˜ å°„åˆ° [-1,1] çš„å¾—åˆ†ï¼ˆå¯æŒ‰éœ€æ‰©å±•ï¼‰
_ECF_RATING2SCORE = {
    "å¼ºçƒˆæ¨è": 1.0, "ä¹°å…¥": 0.9, "å¢æŒ": 0.6, "æ¨è": 0.7, "å®¡æ…å¢æŒ": 0.5,
    "è·‘èµ¢è¡Œä¸š": 0.5, "æŒæœ‰": 0.0, "ä¸­æ€§": 0.0, "è°¨æ…æ¨è": 0.2,
    "å‡æŒ": -0.6, "å–å‡º": -0.9, "è·‘è¾“è¡Œä¸š": -0.5
}
# ç ”æŠ¥æœ€ç»ˆå¾—åˆ† = è¯„çº§åˆ† * Î± + æ–‡æœ¬æƒ…æ„Ÿåˆ† * (1-Î±)
REPORT_ALPHA_RATING = 0.6

# â€”â€” ä¸‹è½½/è°ƒè¯•ç›¸å…³ï¼ˆå¯é€‰ï¼‰ â€”â€”
VERIFY_SSL = True          # å¦‚ä½ åœ¨å†…ç½‘è¢«æ‹¦ï¼Œå¯æš‚æ—¶ç½® Falseï¼ˆä¸æ¨èï¼‰
DEBUG_SAVE_RAW = True      # æŠŠæ‹¿åˆ°çš„ PDF/HTML åŸå§‹å†…å®¹è½ç›˜ä¾¿äºæ’æŸ¥
DEBUG_SAVE_OK_PDF = False  # True=è¿æˆåŠŸçš„ PDF ä¹Ÿè½ç›˜

# å¯èƒ½å¯åŠ¨OCR
ENABLE_OCR = True
OCR_MAX_PAGES = 3  # åª OCR å‰å‡ é¡µä»¥æ§æ—¶

VERIFY_SSL = True              # å¦‚ç¡®å®è¢«å†…ç½‘æ‹¦ï¼Œå¯ä¸´æ—¶ Falseï¼ˆä¸æ¨èï¼‰
DEBUG_SAVE_RAW = True          # å¤±è´¥æ—¶æŠŠ HTML/PDF åŸå§‹å­—èŠ‚è½ç›˜ä»¥ä¾¿æ’æŸ¥
DEBUG_SAVE_OK_PDF = False      # True=è¿æˆåŠŸçš„ PDF ä¹Ÿä¿å­˜ï¼ˆè°ƒè¯•ç”¨ï¼‰
ENABLE_OCR = True              # å¯ç”¨ OCR å…œåº•
OCR_MAX_PAGES = 3              # ä»… OCR å‰å‡ é¡µæ§åˆ¶è€—æ—¶

# â€”â€” ç ”æŠ¥ï¼ˆä¸ªè‚¡ç ”æŠ¥ï¼‰ç¼“å­˜å·¥å…· â€”â€”
def _log_fail(msg: str):
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(FAIL_LOG, "a", encoding="utf-8") as f:
        f.write(f"[{ts}] {msg}\n")

def _sanitize_text(txt: str) -> str:
    if not txt:
        return ""
    # å»è„šæ³¨/é‡å¤ç©ºç™½
    txt = re.sub(r"\s+", " ", txt)
    # å¸¸è§ç‰ˆæƒå°¾å·´ç²—ç•¥å‰”é™¤ï¼ˆå¯æŒ‰éœ€æ‰©å±•ï¼‰
    txt = re.sub(r"(è´£ä»»ç¼–è¾‘[:ï¼š].*?$)", "", txt)
    return txt.strip()

def _report_cache_path(key: str) -> str:
    """ç ”æŠ¥æ–‡æœ¬ç¼“å­˜ï¼šä»¥ {code}_report_{YYYYMMDD_HHMMSS}_{seq}.json å‘½å"""
    fname = f"{key}.json"
    return os.path.join(AKSHARE_STOCK_REPORT_EM_TEXT_DIR, fname)

def _read_report_cache(key: str):
    path = _report_cache_path(key)
    if not os.path.exists(path):
        return None
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        ts = datetime.fromisoformat(data.get("fetched_at"))
        if (datetime.now() - ts).total_seconds() > MAX_AGE_HOURS_REPORT * 3600:
            return None
        return data.get("text", "")
    except Exception:
        return None

def _write_report_cache(key: str, text: str):
    path = _report_cache_path(key)
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump({"key": key, "text": text, "fetched_at": datetime.now().isoformat()}, f, ensure_ascii=False)
    except Exception as e:
        _log_fail(f"WRITE_REPORT_CACHE_FAIL {key} -> {e}")

# â€”â€” PDF ä¸‹è½½ä¸æ–‡æœ¬æŠ½å–å·¥å…· â€”â€”
def _get_session():
    s = requests.Session()
    s.headers.update(REQ_HEADERS)
    retry = Retry(
        total=2, backoff_factor=0.4,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET"]
    )
    s.mount("https://", HTTPAdapter(max_retries=retry))
    s.mount("http://", HTTPAdapter(max_retries=retry))
    return s

def _debug_dump_bytes(content: bytes, url: str, suffix: str):
    if not DEBUG_SAVE_RAW or not content:
        return
    md = hashlib.md5(url.encode("utf-8")).hexdigest()[:10]
    path = os.path.join(AKSHARE_STOCK_REPORT_EM_TEXT_DIR, f"_raw_{md}{suffix}")
    try:
        with open(path, "wb") as f:
            f.write(content)
    except Exception:
        pass

def _requests_get(url: str):
    if not _HAS_REQUESTS:
        raise RuntimeError("Missing 'requests' / 'beautifulsoup4' dependency")
    return requests.get(url, headers=REQ_HEADERS, timeout=REQ_TIMEOUT)

def _is_pdf_bytes(b: bytes) -> bool:
    return isinstance(b, (bytes, bytearray)) and len(b) > 5 and b[:5] == b"%PDF-"

def _download_pdf_bytes(url: str) -> bytes:
    """
    æ™ºèƒ½ä¸‹è½½å™¨ï¼š
    1) curl_cffiï¼ˆChrome æŒ‡çº¹ï¼‰+ é¢„çƒ­ Cookie
    2) tls_clientï¼ˆChrome æŒ‡çº¹ï¼‰+ é¢„çƒ­ Cookie
    3) å›é€€ requestsï¼ˆä½ åŸæœ¬çš„é€»è¾‘ï¼‰
    å…¨ç¨‹å¸¦ Referer/Origin/Acceptï¼Œå¤±è´¥è½ç›˜ HTML ä¾¿äºæ’æŸ¥
    """
    # é€šç”¨å¤´
    h2 = {
        "Referer": "https://data.eastmoney.com/report/stock.jshtml",
        "Origin": "https://data.eastmoney.com",
        "Accept": "application/pdf,text/html;q=0.9,*/*;q=0.8",
        "Connection": "keep-alive",
    }

    # ---------- 1) curl_cffi ----------
    if _HAS_CURL_CFFI:
        try:
            s = cf_requests.Session(impersonate="chrome")
            # é¢„çƒ­ï¼šè·å– Cookie
            s.get("https://data.eastmoney.com/report/stock.jshtml",
                  headers=REQ_HEADERS, timeout=REQ_TIMEOUT, verify=VERIFY_SSL)
            r = s.get(url, headers={**REQ_HEADERS, **h2},
                      timeout=REQ_TIMEOUT, verify=VERIFY_SSL, allow_redirects=True)
            c = r.content or b""
            ct = (r.headers.get("Content-Type") or "").lower()
            if _is_pdf_bytes(c):
                if DEBUG_SAVE_OK_PDF: _debug_dump_bytes(c, url, ".pdf")
                return c
            else:
                if DEBUG_SAVE_RAW and c and ("pdf" not in ct):
                    _debug_dump_bytes(c, url, ".html")
        except Exception as e:
            _log_fail(f"CURL_CFFI_DOWNLOAD_FAIL {url} -> {e}")

    # ---------- 2) tls_client ----------
    if _HAS_TLS_CLIENT:
        try:
            s = tls_client.Session(client_identifier="chrome_120")
            s.headers.update(REQ_HEADERS)
            s.get("https://data.eastmoney.com/report/stock.jshtml",
                  timeout=REQ_TIMEOUT, allow_redirects=True, verify=VERIFY_SSL)
            r = s.get(url, headers=h2, timeout=REQ_TIMEOUT,
                      allow_redirects=True, verify=VERIFY_SSL)
            c = r.content or b""
            ct = (r.headers.get("Content-Type") or "").lower()
            if _is_pdf_bytes(c):
                if DEBUG_SAVE_OK_PDF: _debug_dump_bytes(c, url, ".pdf")
                return c
            else:
                if DEBUG_SAVE_RAW and c and ("pdf" not in ct):
                    _debug_dump_bytes(c, url, ".html")
        except Exception as e:
            _log_fail(f"TLS_CLIENT_DOWNLOAD_FAIL {url} -> {e}")

    # ---------- 3) å›é€€ï¼šrequestsï¼ˆä¿ç•™ä½ åŸé€»è¾‘ï¼‰ ----------
    if not _HAS_REQUESTS:
        raise RuntimeError("Missing 'requests' for PDF download")

    # ç®€æ˜“ä¼šè¯ + é‡è¯•
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
    sess = requests.Session()
    sess.headers.update(REQ_HEADERS)
    retry = Retry(total=2, backoff_factor=0.4,
                  status_forcelist=[429,500,502,503,504], allowed_methods=["GET"])
    sess.mount("https://", HTTPAdapter(max_retries=retry))
    sess.mount("http://", HTTPAdapter(max_retries=retry))

    # ç¬¬ä¸€æ¬¡
    r1 = sess.get(url, timeout=REQ_TIMEOUT, allow_redirects=True, verify=VERIFY_SSL)
    c1 = r1.content or b""
    ct1 = (r1.headers.get("Content-Type") or "").lower()
    if _is_pdf_bytes(c1):
        if DEBUG_SAVE_OK_PDF: _debug_dump_bytes(c1, url, ".pdf")
        return c1
    else:
        if DEBUG_SAVE_RAW and c1 and ("pdf" not in ct1):
            _debug_dump_bytes(c1, url, ".html")

    # ç¬¬äºŒæ¬¡ï¼šåŠ  Referer/Origin
    r2 = sess.get(url, headers=h2, timeout=REQ_TIMEOUT, allow_redirects=True, verify=VERIFY_SSL)
    c2 = r2.content or b""
    ct2 = (r2.headers.get("Content-Type") or "").lower()
    if _is_pdf_bytes(c2):
        if DEBUG_SAVE_OK_PDF: _debug_dump_bytes(c2, url, ".pdf")
        return c2
    else:
        if DEBUG_SAVE_RAW and c2 and ("pdf" not in ct2):
            _debug_dump_bytes(c2, url, ".html")

    # è‹¥ HTMLï¼Œå°è¯•ä»ä¸­æå–çœŸå® .pdf å†æ‹‰
    try:
        enc = r2.apparent_encoding or r2.encoding or "utf-8"; r2.encoding = enc
        html = r2.text or ""
    except Exception:
        html = ""
    if html:
        m = re.search(r"https?://[^\"'>]+\.pdf", html, flags=re.I)
        if m:
            real_pdf = m.group(0)
            r3 = sess.get(real_pdf, headers=h2, timeout=REQ_TIMEOUT, allow_redirects=True, verify=VERIFY_SSL)
            c3 = r3.content or b""
            ct3 = (r3.headers.get("Content-Type") or "").lower()
            if _is_pdf_bytes(c3):
                if DEBUG_SAVE_OK_PDF: _debug_dump_bytes(c3, real_pdf, ".pdf")
                return c3
            else:
                _log_fail(f"REPORT_PDF_NOT_PDF_AFTER_REDIRECT {real_pdf} -> {ct3}")
                if DEBUG_SAVE_RAW and c3: _debug_dump_bytes(c3, real_pdf, ".html")
        else:
            _log_fail(f"REPORT_PDF_HTML_RETURNED_NO_PDF_LINK {url}")
    else:
        _log_fail(f"REPORT_PDF_NOT_PDF {url} -> {ct2}")

    _log_fail(f"REPORT_PDF_DOWNLOAD_FAIL_NOT_PDF {url}")
    return b""


def _ocr_pdf_bytes_with_tesseract(pdf_bytes: bytes, max_pages: int = 3) -> str:
    if not (_HAS_PYMUPDF):
        return ""
    try:
        import pytesseract
        from PIL import Image
        doc = fitz.open(stream=pdf_bytes, filetype="pdf")
        texts = []
        for i, page in enumerate(doc):
            if i >= max_pages:
                break
            pix = page.get_pixmap(dpi=200)  # æé«˜ä¸€ç‚¹æ¸…æ™°åº¦
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            t = pytesseract.image_to_string(img, lang="chi_sim")
            texts.append(t or "")
        return _sanitize_text("\n".join(texts))
    except Exception as e:
        _log_fail(f"OCR_FAIL -> {e}")
        return ""
def _extract_text_from_pdf_bytes(pdf_bytes: bytes, max_chars: int = REPORT_TEXT_MAX_CHARS) -> str:
    """ä¼˜å…ˆ pdfminerï¼›å¤±è´¥æˆ–è¿‡çŸ­å†ç”¨ PyMuPDF å¤šæ¨¡å¼ï¼›æœ€åæ¸…æ´— + æˆªæ–­"""
    if not pdf_bytes:
        return ""

    text_all = ""

    # A) pdfminerï¼ˆç‰ˆé¢æ¨¡å¼å¯¹ä¸­æ–‡ç ”æŠ¥é€šå¸¸è¾ƒç¨³ï¼‰
    if _HAS_PDFMINER:
        try:
            import io
            from pdfminer.high_level import extract_text_to_fp
            from pdfminer.layout import LAParams

            output = io.StringIO()
            laparams = LAParams(
                all_texts=True,
                line_margin=0.2,
                char_margin=1.0,
                word_margin=0.1,
                boxes_flow=None
            )
            extract_text_to_fp(io.BytesIO(pdf_bytes), output, laparams=laparams, output_type='text', codec='utf-8')
            txt = output.getvalue()
            text_all = (txt or "").replace("\x00", "")
        except Exception as e:
            _log_fail(f"PDFMINER_EXTRACT_FAIL -> {e}")
            text_all = ""

    # B) è‹¥ pdfminer ä¸ºç©ºæˆ–å¾ˆçŸ­ï¼Œå°è¯• PyMuPDF å¤šæ¨¡å¼
    if (not text_all or len(text_all) < 50) and _HAS_PYMUPDF:
        try:
            doc = fitz.open(stream=pdf_bytes, filetype="pdf")
            # è§£å¯†ï¼ˆç©ºå¯†ç è¯•ä¸€ä¸‹ï¼‰
            if doc.is_encrypted:
                try:
                    doc.authenticate("")
                except Exception:
                    pass

            parts = []
            # å…ˆç”¨ 'text'ï¼ˆå¸ƒå±€é‡æ’ï¼‰
            for i, page in enumerate(doc):
                t = page.get_text("text") or ""
                parts.append(t)
                if sum(len(p) for p in parts) >= max_chars:
                    break
            t1 = "\n".join(parts).replace("\x00", "").strip()

            # å†ç”¨ 'blocks'ï¼ˆæŒ‰å—ï¼‰ï¼Œä¸¤ä¸ªæ‹¼è°é•¿ç”¨è°
            parts2 = []
            for i, page in enumerate(doc):
                t = page.get_text("blocks") or ""
                # blocks è¿”å›çš„æ˜¯åˆ—è¡¨æ—¶ä¹Ÿå…¼å®¹
                if isinstance(t, list):
                    t = "\n".join(block[4] for block in t if isinstance(block, (list, tuple)) and len(block) >= 5)
                parts2.append(t or "")
                if sum(len(p) for p in parts2) >= max_chars:
                    break
            t2 = "\n".join(parts2).replace("\x00", "").strip()

            text_all = t1 if len(t1) >= len(t2) else t2
        except Exception as e:
            _log_fail(f"PYMUPDF_EXTRACT_FAIL -> {e}")

    # æ¸…æ´— + æˆªæ–­ï¼ˆå…ˆåšä¸€æ¬¡ï¼‰
    text_all = _sanitize_text(text_all)

    # â€”â€” å¦‚æœè¿˜æ˜¯å¾ˆçŸ­/ä¸ºç©ºï¼Œå°è¯• OCR å…œåº• â€”â€”
    if ENABLE_OCR and (not text_all or len(text_all) < 50) and _HAS_PYMUPDF:
        try:
            ocr_text = _ocr_pdf_bytes_with_tesseract(pdf_bytes, max_pages=OCR_MAX_PAGES)
            ocr_text = _sanitize_text(ocr_text)
            if len(ocr_text) > len(text_all):
                text_all = ocr_text
        except Exception as e:
            _log_fail(f"OCR_PIPELINE_FAIL -> {e}")

    # æœ€ç»ˆæˆªæ–­
    if len(text_all) > max_chars:
        text_all = text_all[:max_chars]
    return text_all


# =================== å·¥å…·å‡½æ•° ===================
def _random_pause():
    low, high = RANDOM_SLEEP_RANGE
    time.sleep(random.uniform(low, high))

def _format_pub_time(val) -> str:
    """
    å°†å‘å¸ƒæ—¶é—´å€¼ç»Ÿä¸€æ ¼å¼åŒ–ä¸º YYYYMMDD_HHMMSSï¼›å¤±è´¥åˆ™è¿”å› 'unknown'
    """
    try:
        ts = pd.to_datetime(val, errors="coerce")
        if pd.isna(ts):
            return "unknown"
        return ts.strftime("%Y%m%d_%H%M%S")
    except Exception:
        return "unknown"
def _retry(fn, tries=RETRIES, base_wait=BASE_WAIT):
    last_err = None
    for i in range(tries):
        try:
            return fn()
        except Exception as e:
            last_err = e
            if i < tries - 1:
                time.sleep(base_wait * (2 ** i) + random.random() * 0.3)
    raise last_err
def _get_first_existing_col(df: pd.DataFrame, candidates):
    for c in candidates:
        if c in df.columns:
            return c
    return None

def _need_refresh(path: str, max_age_hours: int) -> bool:
    if not os.path.exists(path):
        return True
    try:
        # mtime = datetime.fromtimestamp(os.path.getmtime(path))
        # return (datetime.now() - mtime).total_seconds() > max_age_hours * 3600
        return True
    except Exception:
        return True
def get_research_report_em_score(code: str, topk: int = REPORT_TOPK) -> float:
    """
    ä¸œæ–¹è´¢å¯Œ-ä¸ªè‚¡ç ”æŠ¥ï¼šç»¼åˆâ€œä¸œè´¢è¯„çº§â€ä¸ PDF æ–‡æœ¬æƒ…æ„Ÿï¼Œå¾—åˆ° [-1,1] çš„ç ”æŠ¥å¾—åˆ†ã€‚
    - åˆ—è¡¨ç¼“å­˜ï¼š{AKSHARE_STOCK_REPORT_EM_DIR}/{code}.csvï¼ˆ24h ä¸ NEWS åŒé€»è¾‘å¯å¤ç”¨ï¼‰
    - æ–‡æœ¬ç¼“å­˜ï¼š{AKSHARE_STOCK_REPORT_EM_TEXT_DIR}/{code}_report_{YYYYMMDD_HHMMSS}_{seq}.json
    """
    path = os.path.join(AKSHARE_STOCK_REPORT_EM_DIR, f"{code}.csv")

    def fetch():
        # ak æ¥å£ symbol éœ€è¦ä»£ç ï¼›è‹¥éœ€æŒ‰åç§°æ£€ç´¢ï¼Œå¯è‡ªè¡Œåœ¨å¤–å±‚åšæ˜ å°„
        return ak.stock_research_report_em(symbol=code)

    # è¯»å–æˆ–åˆ·æ–°åˆ—è¡¨ç¼“å­˜ï¼ˆä¸æ–°é—»ç›¸åŒç­–ç•¥ï¼‰
    df = None
    try:
        if _need_refresh(path, MAX_AGE_HOURS_NEWS):
            df = _retry(fetch)
            if isinstance(df, pd.DataFrame) and not df.empty:
                df.to_csv(path, index=False, encoding="utf-8-sig")
        else:
            df = pd.read_csv(path)
    except Exception as e:
        _log_fail(f"REPORT_LIST_FETCH_FAIL {code} -> {e}")
        if os.path.exists(path):
            try:
                df = pd.read_csv(path)
            except Exception:
                df = None

    if df is None or df.empty:
        return 0.0

    # åˆ—åé²æ£’
    title_col  = _get_first_existing_col(df, ["æŠ¥å‘Šåç§°", "æ ‡é¢˜", "name", "æŠ¥å‘Šæ ‡é¢˜"])
    rating_col = _get_first_existing_col(df, ["ä¸œè´¢è¯„çº§", "è¯„çº§", "rating"])
    date_col   = _get_first_existing_col(df, ["æ—¥æœŸ", "time", "å‘å¸ƒæ—¶é—´", "pub_time"])
    pdf_col    = _get_first_existing_col(df, ["æŠ¥å‘ŠPDFé“¾æ¥", "pdfé“¾æ¥", "pdf", "url"])

    # ä»…ä¿ç•™â€œè¿‘ä¸¤å¹´â€çš„ç ”æŠ¥ï¼Œç„¶åæ’åºå–æœ€è¿‘ topk
    if date_col:
        try:
            df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
        except Exception:
            pass

        # è®¡ç®—æ—¶é—´çª—ï¼šå½“å‰æ—¶åˆ»å¾€å› REPORT_WINDOW_YEARS å¹´ï¼Œåªè·å–è¿‘å¹´æ¥çš„ç ”æŠ¥æ•°æ®
        cutoff = pd.Timestamp.now() - pd.DateOffset(years=REPORT_WINDOW_YEARS)
        # ä¸¢æ‰æ²¡æœ‰æ—¥æœŸçš„ã€ä»¥åŠæ—©äºçª—å£çš„
        df = df[df[date_col].notna() & (df[date_col] >= cutoff)]

        # è‹¥è¿‡æ»¤åä¸ºç©ºï¼Œç›´æ¥è¿”å› 0ï¼ˆä¹Ÿå¯è®°å½•ä¸€æ¡æ—¥å¿—ä¾¿äºæ’æŸ¥ï¼‰
        if df.empty:
            # _log_fail(f"REPORT_DATE_FILTER_EMPTY {code} cutoff={cutoff.date()}")
            return 0.0

        # å‡åºæ’åºï¼Œä¾¿äº tail(topk) å–æœ€è¿‘
        df = df.sort_values(date_col, ascending=True)

    # å–æœ€è¿‘ topk
    df_recent = df.tail(topk) if len(df) > topk else df

    # ç®€å•å»é‡ï¼ˆä¼˜å…ˆæŒ‰ PDF é“¾æ¥ï¼‰
    if pdf_col and pdf_col in df_recent.columns:
        df_recent = df_recent.drop_duplicates(subset=[pdf_col])
    else:
        keys = [c for c in [title_col, date_col] if c and c in df_recent.columns]
        if keys:
            df_recent = df_recent.drop_duplicates(subset=keys)

    item_scores = []

    for i, (_, row) in enumerate(df_recent.iterrows(), start=1):
        # è¯„çº§ â†’ åˆ†å€¼ï¼ˆ[-1,1]ï¼‰
        rating_score = None
        if rating_col and pd.notna(row.get(rating_col, None)):
            raw = str(row[rating_col]).strip()
            # å…¼å®¹â€œä¹°å…¥ï¼ˆé¦–æ¬¡ï¼‰/ç»´æŒä¹°å…¥/ä¸Šè°ƒè‡³å¢æŒâ€ç­‰
            key_hit = None
            for k in _ECF_RATING2SCORE.keys():
                if k in raw:
                    key_hit = k
                    break
            if key_hit is not None:
                rating_score = _ECF_RATING2SCORE[key_hit]

        # æ–‡æœ¬æƒ…æ„Ÿ
        text_sent = None
        # å…ˆ PDF æ­£æ–‡
        pdf_url = str(row.get(pdf_col, "")).strip() if pdf_col else ""
        date_val = row.get(date_col, None)
        pub_key = f"{code}_report_{_format_pub_time(date_val)}_{i:03d}"

        if pdf_url.startswith("http"):
            try:
                cached = _read_report_cache(pub_key)
                if cached:
                    text = cached
                else:
                    pdf_bytes = _retry(lambda: _download_pdf_bytes(pdf_url))
                    if not _is_pdf_bytes(pdf_bytes):
                        _log_fail(f"REPORT_PDF_BYTES_NOT_PDF {code} {pdf_url}")
                        text = ""
                    else:
                        text = _extract_text_from_pdf_bytes(pdf_bytes, max_chars=REPORT_TEXT_MAX_CHARS)
                        print(text)
                        if text:
                            _write_report_cache(pub_key, text)
                if text and len(text) >= MIN_ARTICLE_CHARS:
                    s = SnowNLP(text).sentiments
                    text_sent = (s - 0.5) * 2
            except Exception as e:
                _log_fail(f"REPORT_PDF_PARSE_FAIL {code} {pdf_url} -> {e}")

        # è‹¥ PDF å¤±è´¥ï¼Œå›é€€æ ‡é¢˜æƒ…æ„Ÿ
        if text_sent is None and title_col and pd.notna(row.get(title_col, None)):
            try:
                t = _sanitize_text(str(row[title_col]))
                if t:
                    s = SnowNLP(t).sentiments
                    text_sent = (s - 0.5) * 2
            except Exception as e:
                _log_fail(f"REPORT_TITLE_SENT_FAIL {code} -> {e}")

        # èåˆï¼šè¯„çº§/æ–‡æœ¬å‡æ— åˆ™è·³è¿‡
        if rating_score is None and text_sent is None:
            continue
        if rating_score is None:
            final = text_sent
        elif text_sent is None:
            final = rating_score
        else:
            final = REPORT_ALPHA_RATING * rating_score + (1 - REPORT_ALPHA_RATING) * text_sent

        item_scores.append(final)

    if not item_scores:
        return 0.0
    return round(float(pd.Series(item_scores).mean()), 4)

def _one_code(code: str, name: str = None) -> dict:
    # ç ”æŠ¥æƒ…ç»ªï¼ˆä¸œè´¢ï¼‰
    try:
        senti_report_em = get_research_report_em_score(code, topk=REPORT_TOPK)
    except Exception as e:
        _log_fail(f"AKSHARE_STOCK_REPORT_EM_FAIL {code} -> {e}")
        senti_report_em = 0.0

    _random_pause()

    return {
        "ä»£ç ": code,
        "AKSHAREä¸œè´¢æ¥å£çš„ç ”æŠ¥æƒ…ç»ª": senti_report_em,
    }

def extract_AKSHARE_STOCK_NEWS_EM_fund_features(code_list, max_workers: int = MAX_WORKERS) -> pd.DataFrame:
    """
    code_list æ”¯æŒä¸¤ç§å…ƒç´ å½¢æ€ï¼š
    - "000001"ï¼ˆåªæœ‰ä»£ç ï¼‰
    - ("000001", "å¹³å®‰é“¶è¡Œ")ï¼ˆä»£ç  + åç§°ï¼‰
    """
    rows = []
    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        futs = {}
        for item in code_list:
            if isinstance(item, (list, tuple)) and len(item) >= 2:
                code, name = item[0], item[1]
            else:
                code, name = item, None
            fut = ex.submit(_one_code, code, name)
            futs[fut] = code

        for fut in tqdm(as_completed(futs), total=len(futs), desc="Extracting"):
            try:
                rows.append(fut.result())
            except Exception as e:
                code = futs.get(fut, "UNKNOWN")
                _log_fail(f"TASK_FAIL {code} -> {e}")

    df = pd.DataFrame(rows)
    df.insert(0, "ç”Ÿæˆæ—¶é—´", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    df.insert(1, "ç ”æŠ¥topk", REPORTS_TOPK)
    # df.insert(2, "èµ„é‡‘å¤©æ•°", FUND_DAYS)
    # df.insert(3, "é¾™è™æ¦œå¤©æ•°", LHB_DAYS)
    return df

# =================== ä¸»ç¨‹åº ===================
if __name__ == '__main__':
    # ç¤ºä¾‹ï¼šè·‘ä¸€éè‚¡ç¥¨åˆ—è¡¨ï¼ˆéœ€åŒ…å«â€œä»£ç â€åˆ—ï¼›å»ºè®®å­—ç¬¦ä¸²ç±»å‹ä¿ç•™å‰å¯¼0ï¼‰
    stock_list_path = os.path.join(META_DIR, "stock_list.csv")
    if not os.path.exists(stock_list_path):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°è‚¡ç¥¨åˆ—è¡¨ï¼š{stock_list_path}ï¼Œè¯·å…ˆå‡†å¤‡åŒ…å«â€˜ä»£ç â€™åˆ—çš„CSVã€‚")

    df_list = pd.read_csv(stock_list_path, dtype=str)
    # è·å–è‚¡ç¥¨ä»£ç åˆ—
    if "ä»£ç " not in df_list.columns:
        # åšä¸€ä¸ªå…œåº•ï¼šå¸¸è§å­—æ®µå candidates
        cand = _get_first_existing_col(df_list, ["ä»£ç ", "symbol", "ts_code", "code"])
        if cand and cand != "ä»£ç ":
            df_list.rename(columns={cand: "ä»£ç "}, inplace=True)
        if "ä»£ç " not in df_list.columns:
            raise ValueError("è‚¡ç¥¨åˆ—è¡¨ç¼ºå°‘â€˜ä»£ç â€™åˆ—ï¼ˆæˆ–å¸¸è§ç­‰ä»·åˆ—ï¼‰ã€‚")

    codes = df_list["ä»£ç "].dropna().astype(str).unique().tolist()

    # è·å–è‚¡ç¥¨åç§°åˆ—ï¼ˆå¯é€‰ï¼‰ï¼š['åç§°','è‚¡ç¥¨åç§°','è‚¡ç¥¨ç®€ç§°','name']
    name_col = _get_first_existing_col(df_list, ["åç§°", "è‚¡ç¥¨åç§°", "è‚¡ç¥¨ç®€ç§°", "name"])
    if name_col and name_col in df_list.columns:
        # ç»„è£… (code, name) åˆ—è¡¨
        merged = (
            df_list.dropna(subset=["ä»£ç "])
                .astype({"ä»£ç ": str})
        )
        # æ¯ä¸ªä»£ç åªå–ç¬¬ä¸€æ¡åç§°
        merged = merged.drop_duplicates(subset=["ä»£ç "])
        code_name_list = list(zip(merged["ä»£ç "].tolist(), merged[name_col].astype(str).tolist()))
    else:
        code_name_list = codes  # æ²¡æœ‰åç§°åˆ—å°±åªç”¨ä»£ç 

    df_feat = extract_AKSHARE_STOCK_NEWS_EM_fund_features(code_name_list, max_workers=MAX_WORKERS)
    df_feat.to_csv(OUT_PATH, index=False, encoding="utf-8-sig")
    print(df_feat.head())
    print(f"\nâœ… å·²ä¿å­˜ï¼š{OUT_PATH}")
    print(f"ğŸ“„ å¤±è´¥æ—¥å¿—ï¼ˆå¦‚æœ‰ï¼‰ï¼š{FAIL_LOG}")

