# é€šè¿‡åŒèŠ±é¡ºwencaié—®è´¢è·å–ä¸ªè‚¡ç ”æŠ¥ä¿¡æ¯å¹¶ä¿å­˜ã€æ”¯æŒå¢é‡æ›´æ–°
import os
import time
import random
from datetime import datetime, timedelta

import baostock as bs
import pandas as pd
from tenacity import retry, stop_after_attempt, wait_random
from tqdm import tqdm

import requests
import time
import random
from typing import List, Dict
import pandas as pd


# ===========================
#        è·¯å¾„é…ç½®
# ===========================
BASE_DIR = "/workspace/Quant/data_baostock"
REPORT_DIR = os.path.join(BASE_DIR, "data_iwencai_reports")
META_DIR = os.path.join(BASE_DIR, "metadata")
HIST_DIR = os.path.join(BASE_DIR, "stock_hist")

os.makedirs(REPORT_DIR, exist_ok=True)
os.makedirs(META_DIR, exist_ok=True)
os.makedirs(HIST_DIR, exist_ok=True)

FAILED_LIST = []
EMPTY_LIST = []

def bs_login():
    lg = bs.login()
    if lg.error_code != "0":
        raise Exception(f"Baostock login failed: {lg.error_msg}")

def bs_logout():
    bs.logout()

class IwencaiReportClient:
    """
    åŒèŠ±é¡ºé—®è´¢ - ä¸ªè‚¡ç ”æŠ¥æ¥å£å®¢æˆ·ç«¯
    """

    URL = "https://www.iwencai.com/unifiedwap/unified-wap/v1/information/report"
    DETAIL_URL = "https://www.iwencai.com/unifiedwap/unified-wap/v1/information/notice-detail"

    def __init__(self):
        self.headers = {
            "accept": "application/json, text/plain, */*",
            "content-type": "application/x-www-form-urlencoded",
            "origin": "https://www.iwencai.com",
            "referer": "https://www.iwencai.com/unifiedwap/inforesult",
            "user-agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/143.0.0.0 Safari/537.36"
            ),
            # âš ï¸ ä½¿ç”¨ä½ å·²æœ‰çš„å®Œæ•´ Cookieï¼ˆç¤ºä¾‹ï¼‰
            "cookie": (
                "cuc=xx8ezzz2zvmw; "
                "escapename=mx_566796434; "
                "u_name=mx_566796434; "
                "ta_random_userid=ktsynuvtgz; "
                "sess_tk=eyJ0eXAiOiJKV1QiLCJhbGciOiJFUzI1NiIsImtpZCI6InNlc3NfdGtfMSIsImJ0eSI6InNlc3NfdGsifQ."
                "eyJqdGkiOiI2ZTAwYTQ4MzE4MTdjNjlhZGUwZjEzZWY2MjRmMTFkNTEiLCJpYXQiOjE3NjgyMTAwNzYsImV4cCI6"
                "MTc2ODgxNDg3Niwic3ViIjoiNTY2Nzk2NDM0IiwiaXNzIjoidXBhc3MuaXdlbmNhaS5jb20iLCJhdWQiOiIyMDIw"
                "MTExODUyODg5MDcyIiwiYWN0Ijoib2ZjIiwiY3VocyI6IjNjZGIzNWNiOTdmZmQ2Mzk0M2U3OTdiMGJmNzg2NjY4"
                "ZDEzOGNhZGU0Mzg0N2IyMjI1NjkyZTVlYWMzMzA0NmMifQ."
                "4fJThGiPP8-Vsm_HAbRoS9v81mX6jTxa5riLlJDuoLrBPzafG8YzJl2OtrSFtcZeITMRzJvxCZF03Cy1IIw5qw; "
                "ticket=a7f38de4cf7f3dee19d63006e77965c3; "
                "ttype=WEB; "
                "u_ttype=WEB; "
                "other_uid=Ths_iwencai_Xuangu_9t4ghwjskfabmu9iy7daondbz4hnjia0; "
                "user=MDpteF81NjY3OTY0MzQ6Ok5vbmU6NTAwOjU3Njc5NjQzNDo3LDExMTExMTExMTExLDQwOzQ0LDExLDQwOzYs"
                "MSw0MDs1LDEsNDA6MTY6Ojo1NjY3OTY0MzQ6MTc2ODIxMDA3Njo6OjE2MTI5MjUzNDA6NjA0ODAwOjA6MWQ1M"
                "TE0ZjYyZWYxMzBmZGU5YWM2MTcxODgzYTQwMDZlOmRlZmF1bHRfNTow; "
                "u_ukey=A10702B8689642C6BE607730E11E6E4A; "
                "v=A090c19BeiKTqX5e7o9euutl3uhcdKKgPcmnmWFc6oDGeGGWaUQz5k2YN81y; "
                "PHPSESSID=96090d860f932f660d55e0b685d0442d; "
                "userid=326736439; "    # æœ€å¥½æ˜¯æ¯æ¢ä¸€åªè‚¡ç¥¨éšæœºç”Ÿæˆä¸€ä¸ªuserid 566796434
                "utk=3262e41d288421bc9c6340644220039b; "
                "cid=2c9c0c9b495a5e33e6226c22e7cc3ed91768187607; "
                "ComputerID=2c9c0c9b495a5e33e6226c22e7cc3ed91768187607; "
                "u_dpass=2CcKkH00sroyYH%2FsI12MMJvbN4IRtVjK3sUwYW%2F1mHvt%2B8LjA3QxTinPHALynlBfHi80LrSsTFH9a%2B6rtRvqGg%3D%3D; "
                "u_did=233EB705EF0C4596BD1A255BE0753091; "
                "u_uver=1.0.0; "
                "WafStatus=1; "
                "user_status=0"
            ),
        }

        self.cookie_str = self.headers["cookie"]
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    # ==========================================================
    # åé£æ§ï¼šuserid è½¯åˆ‡æ¢
    # ==========================================================
    def _gen_random_userid_like(self, old_userid: str) -> str:
        """
        ç”Ÿæˆä¸€ä¸ªä¸æ—§ userid ä½æ•°ä¸€è‡´çš„éšæœº userid
        """
        n = len(old_userid)
        first = str(random.randint(1, 9))
        rest = "".join(str(random.randint(0, 9)) for _ in range(n - 1))
        return first + rest

    def _replace_userid_in_cookie(self, new_userid: str):
        """
        ä»…æ›¿æ¢ Cookie ä¸­çš„ userid å­—æ®µ
        """
        parts = self.cookie_str.split(";")
        new_parts = []
        replaced = False

        for p in parts:
            p_strip = p.strip()
            if p_strip.startswith("userid="):
                new_parts.append(f" userid={new_userid}")
                replaced = True
            else:
                new_parts.append(p)

        if not replaced:
            new_parts.append(f" userid={new_userid}")

        self.cookie_str = ";".join(new_parts).strip()
        self.session.headers["cookie"] = self.cookie_str

    # ==========================================================
    # å•é¡µç ”æŠ¥æŠ“å–
    # ==========================================================
    def fetch_reports(
        self,
        code: str,
        offset: int = 0,
        size: int = 15,
        dl: int = 120,
        tl: int = 41,
        date_range: str = "",
        sort: str = "",
        cat_id: str = "",
        max_request_count: int = 10
    ) -> Dict:
        payload = {
            "query": code,
            "query_source": "user",
            "size": str(size),
            "offset": str(offset),
            "dl": str(dl),
            "tl": str(tl),
            "range": "",
            "sort": sort,
            "cat_id": cat_id,
            "date_range": date_range,
        }

        resp = self.session.post(
            self.URL,
            data=payload,
            timeout=10,
        )
        resp.raise_for_status()
        data = resp.json()

        # æœ€å¤šé‡å¤è®¿é—®10æ¬¡
        if data.get("status_code") != 0:
            msg = data.get("status_msg", "")
            request_count = 1
            # âš ï¸ iwencai å¸¸è§é£æ§ï¼šä¼ªâ€œç©ºç»“æœâ€
            while "æŸ¥è¯¢ç»“æœä¸ºç©º" in msg and request_count <= max_request_count:
                print(f"âš ï¸ ç ”æŠ¥æ¥å£è¿”å›ç©ºç»“æœï¼Œå°è¯•æ›´æ¢ userid è¿›è¡Œä¸€æ¬¡éªŒè¯ï¼Œå½“å‰ç¬¬{request_count}æ¬¡")

                old_userid = None
                for p in self.cookie_str.split(";"):
                    p = p.strip()
                    if p.startswith("userid="):
                        old_userid = p.split("=", 1)[1]
                        break

                if old_userid:
                    new_userid = self._gen_random_userid_like(old_userid)
                    print(f"   userid: {old_userid} -> {new_userid}")
                    self._replace_userid_in_cookie(new_userid)

                    # ğŸ” retry ä¸€æ¬¡
                    resp2 = self.session.post(
                        self.URL,
                        data=payload,
                        timeout=10,
                    )
                    resp2.raise_for_status()
                    data2 = resp2.json()

                    if data2.get("status_code") == 0:
                        return data2["data"]
                
                request_count += 1
                time.sleep(1)
            
            raise RuntimeError(f"ç ”æŠ¥æ¥å£å¼‚å¸¸ï¼ˆretry {max_request_count}æ¬¡åä»å¤±è´¥ï¼‰ï¼š{msg}")

        return data["data"]

    # ==========================================================
    # å¤šé¡µç ”æŠ¥çˆ¬å–
    # ==========================================================
    def crawl_all_reports(
        self,
        code: str,
        max_pages: int = 10,
        sleep_sec: float = 1.0,
    ) -> List[Dict]:
        all_results = []
        offset = 0

        for _ in range(max_pages):
            data = self.fetch_reports(code=code, offset=offset)
            results = data.get("results", [])

            if not results:
                break

            all_results.extend(results)
            offset += len(results)

            time.sleep(random.uniform(sleep_sec * 0.8, sleep_sec * 1.6))

        return all_results

    def fetch_report_detail(
        self,
        uid: str,
        code: str,
        max_request_count: int = 10
    ) -> Dict:
        """
        æ ¹æ®ç ”æŠ¥ uid è·å–ç ”æŠ¥å…¨æ–‡ï¼ˆcontentï¼‰
        """
        payload = {
            "type": "report",
            "duid": uid,
            "query_source": "guide",
            "query": code,
        }

        resp = self.session.post(
            self.DETAIL_URL,
            data=payload,
            timeout=10,
        )
        resp.raise_for_status()
        data = resp.json()
        print("data:", data)

        request_count = 1
        # å¦‚æœç ”æŠ¥è¯¦æƒ…ä¸ºç©ºï¼Œåå¤ä¿®æ”¹useridè¯·æ±‚ï¼ˆæœ€å¤š10æ¬¡ï¼‰
        while data["data"].get("wordData") is None and request_count <= max_request_count:
            print(f"âš ï¸ è·å–åˆ°çš„ç ”æŠ¥è¯¦æƒ…ä¸ºç©ºï¼Œå°è¯•æ›´æ¢ useridï¼Œå½“å‰ç¬¬{request_count}æ¬¡")
            old_userid = None
            for p in self.cookie_str.split(";"):
                p = p.strip()
                if p.startswith("userid="):
                    old_userid = p.split("=", 1)[1]
                    break

            if old_userid:
                new_userid = self._gen_random_userid_like(old_userid)
                print(f"   userid: {old_userid} -> {new_userid}")
                self._replace_userid_in_cookie(new_userid)

            # ğŸ” retry ä¸€æ¬¡
            resp2 = self.session.post(
                self.DETAIL_URL,
                data=payload,
                timeout=10,
            )
            resp2.raise_for_status()
            data = resp2.json()
            request_count += 1
            time.sleep(1)

        # å¦‚æœæœ‰å…¶å®ƒå¼‚å¸¸çŠ¶æ€ï¼Œä»…é‡å¤è¯·æ±‚ä¸€æ¬¡
        if data.get("status_code") != 0:
            msg = data.get("status_msg", "")

            # iwencai å…¸å‹é£æ§ï¼šè¿”å›ç©º / æ ¡éªŒå¤±è´¥
            if "æŸ¥è¯¢ç»“æœä¸ºç©º" in msg or "æƒé™" in msg:
                print("âš ï¸ ç ”æŠ¥è¯¦æƒ…æ¥å£è§¦å‘æ ¡éªŒï¼Œå°è¯•æ›´æ¢ userid")

                old_userid = None
                for p in self.cookie_str.split(";"):
                    p = p.strip()
                    if p.startswith("userid="):
                        old_userid = p.split("=", 1)[1]
                        break

                if old_userid:
                    new_userid = self._gen_random_userid_like(old_userid)
                    print(f"   userid: {old_userid} -> {new_userid}")
                    self._replace_userid_in_cookie(new_userid)

                    # ğŸ” retry ä¸€æ¬¡
                    resp2 = self.session.post(
                        self.DETAIL_URL,
                        data=payload,
                        timeout=10,
                    )
                    resp2.raise_for_status()
                    data2 = resp2.json()

                    if data2.get("status_code") == 0:
                        return data2["data"]

                raise RuntimeError(f"ç ”æŠ¥è¯¦æƒ…æ¥å£å¼‚å¸¸ï¼ˆretry åä»å¤±è´¥ï¼‰ï¼š{msg}")

        return data["data"]


def get_codes_from_report_hist_dir(pool):
    d = os.path.join(HIST_DIR, pool)
    if not os.path.exists(d):
        return set()

    codes = set()
    for fn in os.listdir(d):
        if fn.endswith(".csv"):
            codes.add(fn.replace(".csv", "").replace("_", "."))
    return codes


def load_last_snapshot(pool):
    path = os.path.join(META_DIR, f"stock_list_{pool}.csv")
    if not os.path.exists(path):
        return set()
    df = pd.read_csv(path)
    return set(df["code"].tolist())

def update_single_stock_reports(
    client: IwencaiReportClient,
    code: str,
    pool: str,
    init_fetch: int = 20,     # é¦–æ¬¡ä¸‹è½½æ¡æ•°
    max_rows: int = 500,      # CSV æœ€å¤§è¡Œæ•°é˜ˆå€¼
):
    """
    å•åªè‚¡ç¥¨ç ”æŠ¥ç»´æŠ¤è§„åˆ™ï¼š
    - ç¬¬ä¸€æ¬¡ï¼šæŠ“æœ€è¿‘ init_fetch æ¡
    - åç»­ï¼šåªå¢é‡è¿½åŠ ï¼Œä¸åˆ æ—§æ•°æ®
    - å½“ CSV è¡Œæ•° > max_rows æ—¶ï¼š
        æŒ‰ publish_time åˆ é™¤æœ€æ—§çš„ï¼Œåªä¿ç•™ max_rows
    """

    save_dir = os.path.join(REPORT_DIR, pool)
    os.makedirs(save_dir, exist_ok=True)

    code_clean = code.replace(".", "_")
    csv_path = os.path.join(save_dir, f"{code_clean}.csv")

    # ======================================================
    # 1. è¯»å–å·²æœ‰ CSVï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    # ======================================================
    old_df = None
    old_uids = set()

    if os.path.exists(csv_path):
        old_df = pd.read_csv(csv_path)
        if not old_df.empty:
            old_df["uid"] = old_df["uid"].astype(str)
            old_uids = set(old_df["uid"])

    # ======================================================
    # 2. æŠ“å–ç ”æŠ¥åˆ—è¡¨
    #    - é¦–æ¬¡ï¼šåªéœ€è¦è¦†ç›– init_fetch
    #    - åç»­ï¼šæŠ“å¤šä¸€ç‚¹ï¼Œé  uid å»é‡
    # ======================================================
    max_pages = max(1, init_fetch // 15 + 1)

    reports = client.crawl_all_reports(
        code=code.split(".")[-1],
        max_pages=max_pages,
    )

    if not reports:
        EMPTY_LIST.append(code)
        return

    # ======================================================
    # 3. å¢é‡æŠ“å–ç ”æŠ¥å…¨æ–‡
    # ======================================================
    new_rows = []

    for r in reports:
        uid = str(r.get("uid"))
        if uid in old_uids:
            continue
        print(r)
        try:
            detail = client.fetch_report_detail(uid, code.split(".")[-1])
            print(detail)
            word = detail.get("wordData", {})

            # row = {
            #     **r,
            #     "uid": uid,
            #     "content": word.get("content", ""),
            # }
            row = {
                "uid": uid,
                "title": word.get("title", ""),
                "organization":  word.get("organize", ""),
                "author":  word.get("researcher", ""),
                "publish_time":  word.get("pubtime", ""),
                "content": word.get("content", ""),
            }
            new_rows.append(row)

        except Exception as e:
            FAILED_LIST.append(code)
            print(f"[ç ”æŠ¥è¯¦æƒ…å¤±è´¥] {code} {uid}: {e}")

    if not new_rows:
        print(f"âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸[å½“å‰è‚¡ç¥¨çˆ¬å–åˆ°çš„ç ”æŠ¥æ•°æ®æ¡æ•°ä¸º0] {code}")
        return

    new_df = pd.DataFrame(new_rows)

    # ======================================================
    # 4. åˆå¹¶æ–°æ—§æ•°æ®ï¼ˆåªè¿½åŠ ï¼‰
    # ======================================================
    if old_df is not None:
        df = pd.concat([old_df, new_df], ignore_index=True)
    else:
        df = new_df

    # ======================================================
    # 5. å»é‡ï¼ˆæœ€ç»ˆå…œåº•ï¼‰
    # ======================================================
    df["uid"] = df["uid"].astype(str)
    df = df.drop_duplicates(subset=["uid"], keep="first")

    # ======================================================
    # 6. è¶…è¿‡é˜ˆå€¼ â†’ åˆ é™¤æœ€æ—§çš„
    # ======================================================
    if "publish_time" in df.columns:
        df["publish_time"] = pd.to_datetime(df["publish_time"], errors="coerce")
        df = df.sort_values("publish_time", ascending=True)  # æ—§ â†’ æ–°

    if len(df) > max_rows:
        df = df.iloc[-max_rows:]   # åªä¿ç•™æœ€æ–° max_rows æ¡

    # ======================================================
    # 7. å†™å› CSV
    # ======================================================
    df.to_csv(csv_path, index=False, encoding="utf-8")

def _bs_query_to_df(rs):
    data_list = []
    while (rs.error_code == "0") & rs.next():
        data_list.append(rs.get_row_data())
    return pd.DataFrame(data_list, columns=rs.fields)

def get_latest_end_date():
    """
    æ ¹æ® Baostock äº¤æ˜“æ—¥å†ï¼Œè·å–è·ç¦»ç¨‹åºè¿è¡Œå½“å¤©æœ€è¿‘çš„ä¸€ä¸ªäº¤æ˜“æ—¥
    - å¦‚æœä»Šå¤©æ˜¯äº¤æ˜“æ—¥ â†’ è¿”å›ä»Šå¤©
    - å¦‚æœä»Šå¤©ä¸æ˜¯äº¤æ˜“æ—¥ï¼ˆå‘¨æœ«/èŠ‚å‡æ—¥ï¼‰â†’ è¿”å›æœ€è¿‘äº¤æ˜“æ—¥
    """

    # ---- ç™»å½• ----
    # lg = bs.login()
    # if lg.error_code != "0":
    #     raise Exception(f"Baostock login failed: {lg.error_msg}")

    today = datetime.now().strftime("%Y-%m-%d")

    # ä»…æŸ¥è¯¢ä»Šå¹´çš„äº¤æ˜“æ—¥å³å¯ï¼ˆå¿«ï¼‰
    year_start = f"{datetime.now().year}-01-01"
    year_second= f"{datetime.now().year}-01-02"
    year_third = f"{datetime.now().year}-01-03"
    year_fourth = f"{datetime.now().year}-01-04"
    
    # å¦‚æœå½“æ—¥æ˜¯æŸå¹´çš„1æœˆ1æ—¥ï¼Œåº”è¯¥æŸ¥è¯¢å‰ä¸€å¹´çš„æœ€åä¸€ä¸ªäº¤æ˜“æ—¥ä¸ºend_dateï¼Œyear_startéœ€è¦ç›¸åº”è°ƒæ•´
    if today == year_start or today == year_second or today == year_third or today ==year_fourth:
        year_start = f"{datetime.now().year - 1}-01-01"
        
    rs = bs.query_trade_dates(start_date=year_start, end_date=today)

    # ---- è§£æäº¤æ˜“æ—¥æ•°æ® ----
    data_list = []
    while (rs.error_code == "0") & rs.next():
        data_list.append(rs.get_row_data())

    # é€€å‡º
    # bs.logout()

    if not data_list:
        raise Exception("baostock query_trade_dates è¿”å›ç©ºæ•°æ®")

    df = pd.DataFrame(data_list, columns=rs.fields)

    # DataFrame å­—æ®µï¼šcalendar_date, is_trading_day
    df["calendar_date"] = pd.to_datetime(df["calendar_date"])
    df["is_trading_day"] = df["is_trading_day"].astype(int)

    # æå–æ‰€æœ‰äº¤æ˜“æ—¥
    trade_days = df[df["is_trading_day"] == 1]["calendar_date"].tolist()

    if len(trade_days) == 0:
        raise Exception("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•äº¤æ˜“æ—¥")

    # æœ€è¿‘ä¸€ä¸ªäº¤æ˜“æ—¥
    latest_trade_day = trade_days[-1].strftime("%Y-%m-%d")
    return latest_trade_day

def get_stock_list_bs(mode="hs300", day=None):
    if mode == "sz50":
        rs = bs.query_sz50_stocks()
    elif mode == "hs300":
        rs = bs.query_hs300_stocks()
    elif mode == "zz500":
        rs = bs.query_zz500_stocks()
    elif mode == "all":
        if day is None:
            raise ValueError("mode='all' éœ€è¦ day='YYYY-MM-DD'")
        print(day)
        # rs = bs.query_all_stock(day="2025-12-22")
        rs = bs.query_all_stock(day=day)
    else:
        raise ValueError(f"æœªçŸ¥è‚¡ç¥¨æ± æ¨¡å¼ï¼š{mode}")

    df = _bs_query_to_df(rs)
    print(df)

    if "code_name" in df.columns:
        df = df.rename(columns={"code_name": "name"})
    else:
        df["name"] = df["code"]

    return df[["code", "name"]]

def run_report_download(pool="hs300"):
    bs_login()
    client = IwencaiReportClient()

    # === è‚¡ç¥¨æ±  ===
    if pool == "all":
        stocks = get_stock_list_bs("all", day=get_latest_end_date())
    else:
        stocks = get_stock_list_bs(pool)

    # stocks.to_csv(
    #     os.path.join(META_DIR, f"stock_list_{pool}.csv"),
    #     index=False,
    #     encoding="utf-8",
    # )

    # snapshot_path = os.path.join(
    #     SNAPSHOT_DIR,
    #     f"{pool}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
    # )
    # stocks.to_csv(snapshot_path, index=False, encoding="utf-8")

    current_codes = set(stocks["code"])
    hist_codes = get_codes_from_report_hist_dir(pool)
    last_snapshot_codes = load_last_snapshot(pool)
    # print(hist_codes)

    final_codes = current_codes | hist_codes | last_snapshot_codes
    codes = sorted(final_codes)

    print(f"[è‚¡ç¥¨æ± ] æœ¬æ¬¡ {len(current_codes)}")
    print(f"[å†å²ç ”æŠ¥] {len(hist_codes)}")
    print(f"[æœ€ç»ˆæ›´æ–°] {len(codes)}")

    for code in tqdm(codes):
        try:
            update_single_stock_reports(client, code, pool)
            time.sleep(random.uniform(0.8, 1.5))
        except Exception as e:
            FAILED_LIST.append(code)
            print(f"[å¤±è´¥] {code}: {e}")

    # ---- æ—¥å¿— ----
    if FAILED_LIST:
        pd.DataFrame({"code": FAILED_LIST}).to_csv(
            os.path.join(REPORT_DIR, "failed_reports.csv"),
            index=False,
        )

    if EMPTY_LIST:
        pd.DataFrame({"code": EMPTY_LIST}).to_csv(
            os.path.join(REPORT_DIR, "empty_reports.csv"),
            index=False,
        )

if __name__ == "__main__":
    # run_report_download(pool="sz50")
    run_report_download(pool="hs300")
    # run_report_download(pool="zz500")
    # run_report_download(pool="all")
    bs_logout()